{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Create Fibonacci sequence using simple RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3XLpEKNQv3X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57e1f841-d6f0-4202-a3b7-918616bcec8d"
      },
      "source": [
        "import numpy as np\n",
        "a = int(input('Give amount: '))\n",
        "\n",
        "def fib(n):\n",
        "    a, b = 0, 1\n",
        "    for _ in range(n):\n",
        "        yield a\n",
        "        a, b = b, a + b\n",
        "\n",
        "fibo = list(fib(a))\n",
        "train_len = len(fibo)*80//100\n",
        "test_len = len(fibo)*10//100 + train_len\n",
        "val_len = len(fibo)*10//100 + test_len\n",
        "train = np.array(fibo[:train_len])\n",
        "test = np.array(fibo[train_len:test_len])\n",
        "val = np.array(fibo[test_len:val_len])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Give amount: 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZP5J-HVRJol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe89cdc0-c9e3-417e-b4f7-bc2812956aed"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "\n",
        "gen_train = TimeseriesGenerator( train, train, length=15, batch_size=5, sampling_rate=1 )\n",
        "gen_test = TimeseriesGenerator( test, test, length=15, batch_size=5, sampling_rate=1 )\n",
        "gen_val = TimeseriesGenerator( val, val, length=15, batch_size=5, sampling_rate=1 )\n",
        "\n",
        "print( \"\\n=== data_gen1: length=12, batch_size=2, sampling_rate=1 ===\" )\n",
        "list_x_train = []\n",
        "list_y_train = []\n",
        "for i in range( len(gen_train) ):\n",
        "    x , y = gen_train[i]\n",
        "    for i in x:\n",
        "      list_x_train.append(list(i))\n",
        "    for i in y:\n",
        "      list_y_train.append(i)\n",
        "    print( \"\\nindex \", i )\n",
        "    print( \"    x: {} {} \\n{}\".format( type(x), x.shape, x ) )\n",
        "    print( \"    y: {} {} \\n{}\".format( type(y), y.shape, y ) )\n",
        "\n",
        "list_x_test = []\n",
        "list_y_test = []\n",
        "for i in range( len(gen_test) ):\n",
        "    x , y = gen_test[i]\n",
        "    for i in x:\n",
        "      list_x_test.append(list(i))\n",
        "    for i in y:\n",
        "      list_y_test.append(i)\n",
        "    print( \"\\nindex \", i )\n",
        "    print( \"    x: {} {} \\n{}\".format( type(x), x.shape, x ) )\n",
        "    print( \"    y: {} {} \\n{}\".format( type(y), y.shape, y ) )\n",
        "\n",
        "list_x_val = []\n",
        "list_y_val = []\n",
        "for i in range( len(gen_val) ):\n",
        "    x , y = gen_val[i]\n",
        "    for i in x:\n",
        "      list_x_val.append(list(i))\n",
        "    for i in y:\n",
        "      list_y_val.append(i)\n",
        "    print( \"\\nindex \", i )\n",
        "    print( \"    x: {} {} \\n{}\".format( type(x), x.shape, x ) )\n",
        "    print( \"    y: {} {} \\n{}\".format( type(y), y.shape, y ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=== data_gen1: length=12, batch_size=2, sampling_rate=1 ===\n",
            "\n",
            "index  4181\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[0 1 1 2 3 5 8 13 21 34 55 89 144 233 377]\n",
            " [1 1 2 3 5 8 13 21 34 55 89 144 233 377 610]\n",
            " [1 2 3 5 8 13 21 34 55 89 144 233 377 610 987]\n",
            " [2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597]\n",
            " [3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[ 610  987 1597 2584 4181]\n",
            "\n",
            "index  46368\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181]\n",
            " [8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765]\n",
            " [13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 10946]\n",
            " [21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 10946 17711]\n",
            " [34 55 89 144 233 377 610 987 1597 2584 4181 6765 10946 17711 28657]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[ 6765 10946 17711 28657 46368]\n",
            "\n",
            "index  514229\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[55 89 144 233 377 610 987 1597 2584 4181 6765 10946 17711 28657 46368]\n",
            " [89 144 233 377 610 987 1597 2584 4181 6765 10946 17711 28657 46368\n",
            "  75025]\n",
            " [144 233 377 610 987 1597 2584 4181 6765 10946 17711 28657 46368 75025\n",
            "  121393]\n",
            " [233 377 610 987 1597 2584 4181 6765 10946 17711 28657 46368 75025\n",
            "  121393 196418]\n",
            " [377 610 987 1597 2584 4181 6765 10946 17711 28657 46368 75025 121393\n",
            "  196418 317811]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[ 75025 121393 196418 317811 514229]\n",
            "\n",
            "index  5702887\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[610 987 1597 2584 4181 6765 10946 17711 28657 46368 75025 121393 196418\n",
            "  317811 514229]\n",
            " [987 1597 2584 4181 6765 10946 17711 28657 46368 75025 121393 196418\n",
            "  317811 514229 832040]\n",
            " [1597 2584 4181 6765 10946 17711 28657 46368 75025 121393 196418 317811\n",
            "  514229 832040 1346269]\n",
            " [2584 4181 6765 10946 17711 28657 46368 75025 121393 196418 317811\n",
            "  514229 832040 1346269 2178309]\n",
            " [4181 6765 10946 17711 28657 46368 75025 121393 196418 317811 514229\n",
            "  832040 1346269 2178309 3524578]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[ 832040 1346269 2178309 3524578 5702887]\n",
            "\n",
            "index  63245986\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[6765 10946 17711 28657 46368 75025 121393 196418 317811 514229 832040\n",
            "  1346269 2178309 3524578 5702887]\n",
            " [10946 17711 28657 46368 75025 121393 196418 317811 514229 832040\n",
            "  1346269 2178309 3524578 5702887 9227465]\n",
            " [17711 28657 46368 75025 121393 196418 317811 514229 832040 1346269\n",
            "  2178309 3524578 5702887 9227465 14930352]\n",
            " [28657 46368 75025 121393 196418 317811 514229 832040 1346269 2178309\n",
            "  3524578 5702887 9227465 14930352 24157817]\n",
            " [46368 75025 121393 196418 317811 514229 832040 1346269 2178309 3524578\n",
            "  5702887 9227465 14930352 24157817 39088169]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[ 9227465 14930352 24157817 39088169 63245986]\n",
            "\n",
            "index  701408733\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[75025 121393 196418 317811 514229 832040 1346269 2178309 3524578\n",
            "  5702887 9227465 14930352 24157817 39088169 63245986]\n",
            " [121393 196418 317811 514229 832040 1346269 2178309 3524578 5702887\n",
            "  9227465 14930352 24157817 39088169 63245986 102334155]\n",
            " [196418 317811 514229 832040 1346269 2178309 3524578 5702887 9227465\n",
            "  14930352 24157817 39088169 63245986 102334155 165580141]\n",
            " [317811 514229 832040 1346269 2178309 3524578 5702887 9227465 14930352\n",
            "  24157817 39088169 63245986 102334155 165580141 267914296]\n",
            " [514229 832040 1346269 2178309 3524578 5702887 9227465 14930352 24157817\n",
            "  39088169 63245986 102334155 165580141 267914296 433494437]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[102334155 165580141 267914296 433494437 701408733]\n",
            "\n",
            "index  7778742049\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[832040 1346269 2178309 3524578 5702887 9227465 14930352 24157817\n",
            "  39088169 63245986 102334155 165580141 267914296 433494437 701408733]\n",
            " [1346269 2178309 3524578 5702887 9227465 14930352 24157817 39088169\n",
            "  63245986 102334155 165580141 267914296 433494437 701408733 1134903170]\n",
            " [2178309 3524578 5702887 9227465 14930352 24157817 39088169 63245986\n",
            "  102334155 165580141 267914296 433494437 701408733 1134903170 1836311903]\n",
            " [3524578 5702887 9227465 14930352 24157817 39088169 63245986 102334155\n",
            "  165580141 267914296 433494437 701408733 1134903170 1836311903\n",
            "  2971215073]\n",
            " [5702887 9227465 14930352 24157817 39088169 63245986 102334155 165580141\n",
            "  267914296 433494437 701408733 1134903170 1836311903 2971215073\n",
            "  4807526976]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[1134903170 1836311903 2971215073 4807526976 7778742049]\n",
            "\n",
            "index  86267571272\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[9227465 14930352 24157817 39088169 63245986 102334155 165580141\n",
            "  267914296 433494437 701408733 1134903170 1836311903 2971215073\n",
            "  4807526976 7778742049]\n",
            " [14930352 24157817 39088169 63245986 102334155 165580141 267914296\n",
            "  433494437 701408733 1134903170 1836311903 2971215073 4807526976\n",
            "  7778742049 12586269025]\n",
            " [24157817 39088169 63245986 102334155 165580141 267914296 433494437\n",
            "  701408733 1134903170 1836311903 2971215073 4807526976 7778742049\n",
            "  12586269025 20365011074]\n",
            " [39088169 63245986 102334155 165580141 267914296 433494437 701408733\n",
            "  1134903170 1836311903 2971215073 4807526976 7778742049 12586269025\n",
            "  20365011074 32951280099]\n",
            " [63245986 102334155 165580141 267914296 433494437 701408733 1134903170\n",
            "  1836311903 2971215073 4807526976 7778742049 12586269025 20365011074\n",
            "  32951280099 53316291173]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[12586269025 20365011074 32951280099 53316291173 86267571272]\n",
            "\n",
            "index  956722026041\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[102334155 165580141 267914296 433494437 701408733 1134903170 1836311903\n",
            "  2971215073 4807526976 7778742049 12586269025 20365011074 32951280099\n",
            "  53316291173 86267571272]\n",
            " [165580141 267914296 433494437 701408733 1134903170 1836311903\n",
            "  2971215073 4807526976 7778742049 12586269025 20365011074 32951280099\n",
            "  53316291173 86267571272 139583862445]\n",
            " [267914296 433494437 701408733 1134903170 1836311903 2971215073\n",
            "  4807526976 7778742049 12586269025 20365011074 32951280099 53316291173\n",
            "  86267571272 139583862445 225851433717]\n",
            " [433494437 701408733 1134903170 1836311903 2971215073 4807526976\n",
            "  7778742049 12586269025 20365011074 32951280099 53316291173 86267571272\n",
            "  139583862445 225851433717 365435296162]\n",
            " [701408733 1134903170 1836311903 2971215073 4807526976 7778742049\n",
            "  12586269025 20365011074 32951280099 53316291173 86267571272\n",
            "  139583862445 225851433717 365435296162 591286729879]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[139583862445 225851433717 365435296162 591286729879 956722026041]\n",
            "\n",
            "index  10610209857723\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[1134903170 1836311903 2971215073 4807526976 7778742049 12586269025\n",
            "  20365011074 32951280099 53316291173 86267571272 139583862445\n",
            "  225851433717 365435296162 591286729879 956722026041]\n",
            " [1836311903 2971215073 4807526976 7778742049 12586269025 20365011074\n",
            "  32951280099 53316291173 86267571272 139583862445 225851433717\n",
            "  365435296162 591286729879 956722026041 1548008755920]\n",
            " [2971215073 4807526976 7778742049 12586269025 20365011074 32951280099\n",
            "  53316291173 86267571272 139583862445 225851433717 365435296162\n",
            "  591286729879 956722026041 1548008755920 2504730781961]\n",
            " [4807526976 7778742049 12586269025 20365011074 32951280099 53316291173\n",
            "  86267571272 139583862445 225851433717 365435296162 591286729879\n",
            "  956722026041 1548008755920 2504730781961 4052739537881]\n",
            " [7778742049 12586269025 20365011074 32951280099 53316291173 86267571272\n",
            "  139583862445 225851433717 365435296162 591286729879 956722026041\n",
            "  1548008755920 2504730781961 4052739537881 6557470319842]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[ 1548008755920  2504730781961  4052739537881  6557470319842\n",
            " 10610209857723]\n",
            "\n",
            "index  117669030460994\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[12586269025 20365011074 32951280099 53316291173 86267571272\n",
            "  139583862445 225851433717 365435296162 591286729879 956722026041\n",
            "  1548008755920 2504730781961 4052739537881 6557470319842 10610209857723]\n",
            " [20365011074 32951280099 53316291173 86267571272 139583862445\n",
            "  225851433717 365435296162 591286729879 956722026041 1548008755920\n",
            "  2504730781961 4052739537881 6557470319842 10610209857723 17167680177565]\n",
            " [32951280099 53316291173 86267571272 139583862445 225851433717\n",
            "  365435296162 591286729879 956722026041 1548008755920 2504730781961\n",
            "  4052739537881 6557470319842 10610209857723 17167680177565\n",
            "  27777890035288]\n",
            " [53316291173 86267571272 139583862445 225851433717 365435296162\n",
            "  591286729879 956722026041 1548008755920 2504730781961 4052739537881\n",
            "  6557470319842 10610209857723 17167680177565 27777890035288\n",
            "  44945570212853]\n",
            " [86267571272 139583862445 225851433717 365435296162 591286729879\n",
            "  956722026041 1548008755920 2504730781961 4052739537881 6557470319842\n",
            "  10610209857723 17167680177565 27777890035288 44945570212853\n",
            "  72723460248141]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[ 17167680177565  27777890035288  44945570212853  72723460248141\n",
            " 117669030460994]\n",
            "\n",
            "index  1304969544928657\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[139583862445 225851433717 365435296162 591286729879 956722026041\n",
            "  1548008755920 2504730781961 4052739537881 6557470319842 10610209857723\n",
            "  17167680177565 27777890035288 44945570212853 72723460248141\n",
            "  117669030460994]\n",
            " [225851433717 365435296162 591286729879 956722026041 1548008755920\n",
            "  2504730781961 4052739537881 6557470319842 10610209857723 17167680177565\n",
            "  27777890035288 44945570212853 72723460248141 117669030460994\n",
            "  190392490709135]\n",
            " [365435296162 591286729879 956722026041 1548008755920 2504730781961\n",
            "  4052739537881 6557470319842 10610209857723 17167680177565\n",
            "  27777890035288 44945570212853 72723460248141 117669030460994\n",
            "  190392490709135 308061521170129]\n",
            " [591286729879 956722026041 1548008755920 2504730781961 4052739537881\n",
            "  6557470319842 10610209857723 17167680177565 27777890035288\n",
            "  44945570212853 72723460248141 117669030460994 190392490709135\n",
            "  308061521170129 498454011879264]\n",
            " [956722026041 1548008755920 2504730781961 4052739537881 6557470319842\n",
            "  10610209857723 17167680177565 27777890035288 44945570212853\n",
            "  72723460248141 117669030460994 190392490709135 308061521170129\n",
            "  498454011879264 806515533049393]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[ 190392490709135  308061521170129  498454011879264  806515533049393\n",
            " 1304969544928657]\n",
            "\n",
            "index  14472334024676221\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[1548008755920 2504730781961 4052739537881 6557470319842 10610209857723\n",
            "  17167680177565 27777890035288 44945570212853 72723460248141\n",
            "  117669030460994 190392490709135 308061521170129 498454011879264\n",
            "  806515533049393 1304969544928657]\n",
            " [2504730781961 4052739537881 6557470319842 10610209857723 17167680177565\n",
            "  27777890035288 44945570212853 72723460248141 117669030460994\n",
            "  190392490709135 308061521170129 498454011879264 806515533049393\n",
            "  1304969544928657 2111485077978050]\n",
            " [4052739537881 6557470319842 10610209857723 17167680177565\n",
            "  27777890035288 44945570212853 72723460248141 117669030460994\n",
            "  190392490709135 308061521170129 498454011879264 806515533049393\n",
            "  1304969544928657 2111485077978050 3416454622906707]\n",
            " [6557470319842 10610209857723 17167680177565 27777890035288\n",
            "  44945570212853 72723460248141 117669030460994 190392490709135\n",
            "  308061521170129 498454011879264 806515533049393 1304969544928657\n",
            "  2111485077978050 3416454622906707 5527939700884757]\n",
            " [10610209857723 17167680177565 27777890035288 44945570212853\n",
            "  72723460248141 117669030460994 190392490709135 308061521170129\n",
            "  498454011879264 806515533049393 1304969544928657 2111485077978050\n",
            "  3416454622906707 5527939700884757 8944394323791464]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[ 2111485077978050  3416454622906707  5527939700884757  8944394323791464\n",
            " 14472334024676221]\n",
            "\n",
            "index  160500643816367088\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[17167680177565 27777890035288 44945570212853 72723460248141\n",
            "  117669030460994 190392490709135 308061521170129 498454011879264\n",
            "  806515533049393 1304969544928657 2111485077978050 3416454622906707\n",
            "  5527939700884757 8944394323791464 14472334024676221]\n",
            " [27777890035288 44945570212853 72723460248141 117669030460994\n",
            "  190392490709135 308061521170129 498454011879264 806515533049393\n",
            "  1304969544928657 2111485077978050 3416454622906707 5527939700884757\n",
            "  8944394323791464 14472334024676221 23416728348467685]\n",
            " [44945570212853 72723460248141 117669030460994 190392490709135\n",
            "  308061521170129 498454011879264 806515533049393 1304969544928657\n",
            "  2111485077978050 3416454622906707 5527939700884757 8944394323791464\n",
            "  14472334024676221 23416728348467685 37889062373143906]\n",
            " [72723460248141 117669030460994 190392490709135 308061521170129\n",
            "  498454011879264 806515533049393 1304969544928657 2111485077978050\n",
            "  3416454622906707 5527939700884757 8944394323791464 14472334024676221\n",
            "  23416728348467685 37889062373143906 61305790721611591]\n",
            " [117669030460994 190392490709135 308061521170129 498454011879264\n",
            "  806515533049393 1304969544928657 2111485077978050 3416454622906707\n",
            "  5527939700884757 8944394323791464 14472334024676221 23416728348467685\n",
            "  37889062373143906 61305790721611591 99194853094755497]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[ 23416728348467685  37889062373143906  61305790721611591\n",
            "  99194853094755497 160500643816367088]\n",
            "\n",
            "index  1779979416004714189\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[190392490709135 308061521170129 498454011879264 806515533049393\n",
            "  1304969544928657 2111485077978050 3416454622906707 5527939700884757\n",
            "  8944394323791464 14472334024676221 23416728348467685 37889062373143906\n",
            "  61305790721611591 99194853094755497 160500643816367088]\n",
            " [308061521170129 498454011879264 806515533049393 1304969544928657\n",
            "  2111485077978050 3416454622906707 5527939700884757 8944394323791464\n",
            "  14472334024676221 23416728348467685 37889062373143906 61305790721611591\n",
            "  99194853094755497 160500643816367088 259695496911122585]\n",
            " [498454011879264 806515533049393 1304969544928657 2111485077978050\n",
            "  3416454622906707 5527939700884757 8944394323791464 14472334024676221\n",
            "  23416728348467685 37889062373143906 61305790721611591 99194853094755497\n",
            "  160500643816367088 259695496911122585 420196140727489673]\n",
            " [806515533049393 1304969544928657 2111485077978050 3416454622906707\n",
            "  5527939700884757 8944394323791464 14472334024676221 23416728348467685\n",
            "  37889062373143906 61305790721611591 99194853094755497\n",
            "  160500643816367088 259695496911122585 420196140727489673\n",
            "  679891637638612258]\n",
            " [1304969544928657 2111485077978050 3416454622906707 5527939700884757\n",
            "  8944394323791464 14472334024676221 23416728348467685 37889062373143906\n",
            "  61305790721611591 99194853094755497 160500643816367088\n",
            "  259695496911122585 420196140727489673 679891637638612258\n",
            "  1100087778366101931]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[ 259695496911122585  420196140727489673  679891637638612258\n",
            " 1100087778366101931 1779979416004714189]\n",
            "\n",
            "index  19740274219868223167\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[2111485077978050 3416454622906707 5527939700884757 8944394323791464\n",
            "  14472334024676221 23416728348467685 37889062373143906 61305790721611591\n",
            "  99194853094755497 160500643816367088 259695496911122585\n",
            "  420196140727489673 679891637638612258 1100087778366101931\n",
            "  1779979416004714189]\n",
            " [3416454622906707 5527939700884757 8944394323791464 14472334024676221\n",
            "  23416728348467685 37889062373143906 61305790721611591 99194853094755497\n",
            "  160500643816367088 259695496911122585 420196140727489673\n",
            "  679891637638612258 1100087778366101931 1779979416004714189\n",
            "  2880067194370816120]\n",
            " [5527939700884757 8944394323791464 14472334024676221 23416728348467685\n",
            "  37889062373143906 61305790721611591 99194853094755497\n",
            "  160500643816367088 259695496911122585 420196140727489673\n",
            "  679891637638612258 1100087778366101931 1779979416004714189\n",
            "  2880067194370816120 4660046610375530309]\n",
            " [8944394323791464 14472334024676221 23416728348467685 37889062373143906\n",
            "  61305790721611591 99194853094755497 160500643816367088\n",
            "  259695496911122585 420196140727489673 679891637638612258\n",
            "  1100087778366101931 1779979416004714189 2880067194370816120\n",
            "  4660046610375530309 7540113804746346429]\n",
            " [14472334024676221 23416728348467685 37889062373143906 61305790721611591\n",
            "  99194853094755497 160500643816367088 259695496911122585\n",
            "  420196140727489673 679891637638612258 1100087778366101931\n",
            "  1779979416004714189 2880067194370816120 4660046610375530309\n",
            "  7540113804746346429 12200160415121876738]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[2880067194370816120 4660046610375530309 7540113804746346429\n",
            " 12200160415121876738 19740274219868223167]\n",
            "\n",
            "index  218922995834555169026\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[23416728348467685 37889062373143906 61305790721611591 99194853094755497\n",
            "  160500643816367088 259695496911122585 420196140727489673\n",
            "  679891637638612258 1100087778366101931 1779979416004714189\n",
            "  2880067194370816120 4660046610375530309 7540113804746346429\n",
            "  12200160415121876738 19740274219868223167]\n",
            " [37889062373143906 61305790721611591 99194853094755497\n",
            "  160500643816367088 259695496911122585 420196140727489673\n",
            "  679891637638612258 1100087778366101931 1779979416004714189\n",
            "  2880067194370816120 4660046610375530309 7540113804746346429\n",
            "  12200160415121876738 19740274219868223167 31940434634990099905]\n",
            " [61305790721611591 99194853094755497 160500643816367088\n",
            "  259695496911122585 420196140727489673 679891637638612258\n",
            "  1100087778366101931 1779979416004714189 2880067194370816120\n",
            "  4660046610375530309 7540113804746346429 12200160415121876738\n",
            "  19740274219868223167 31940434634990099905 51680708854858323072]\n",
            " [99194853094755497 160500643816367088 259695496911122585\n",
            "  420196140727489673 679891637638612258 1100087778366101931\n",
            "  1779979416004714189 2880067194370816120 4660046610375530309\n",
            "  7540113804746346429 12200160415121876738 19740274219868223167\n",
            "  31940434634990099905 51680708854858323072 83621143489848422977]\n",
            " [160500643816367088 259695496911122585 420196140727489673\n",
            "  679891637638612258 1100087778366101931 1779979416004714189\n",
            "  2880067194370816120 4660046610375530309 7540113804746346429\n",
            "  12200160415121876738 19740274219868223167 31940434634990099905\n",
            "  51680708854858323072 83621143489848422977 135301852344706746049]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[31940434634990099905 51680708854858323072 83621143489848422977\n",
            " 135301852344706746049 218922995834555169026]\n",
            "\n",
            "index  2427893228399975082453\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[259695496911122585 420196140727489673 679891637638612258\n",
            "  1100087778366101931 1779979416004714189 2880067194370816120\n",
            "  4660046610375530309 7540113804746346429 12200160415121876738\n",
            "  19740274219868223167 31940434634990099905 51680708854858323072\n",
            "  83621143489848422977 135301852344706746049 218922995834555169026]\n",
            " [420196140727489673 679891637638612258 1100087778366101931\n",
            "  1779979416004714189 2880067194370816120 4660046610375530309\n",
            "  7540113804746346429 12200160415121876738 19740274219868223167\n",
            "  31940434634990099905 51680708854858323072 83621143489848422977\n",
            "  135301852344706746049 218922995834555169026 354224848179261915075]\n",
            " [679891637638612258 1100087778366101931 1779979416004714189\n",
            "  2880067194370816120 4660046610375530309 7540113804746346429\n",
            "  12200160415121876738 19740274219868223167 31940434634990099905\n",
            "  51680708854858323072 83621143489848422977 135301852344706746049\n",
            "  218922995834555169026 354224848179261915075 573147844013817084101]\n",
            " [1100087778366101931 1779979416004714189 2880067194370816120\n",
            "  4660046610375530309 7540113804746346429 12200160415121876738\n",
            "  19740274219868223167 31940434634990099905 51680708854858323072\n",
            "  83621143489848422977 135301852344706746049 218922995834555169026\n",
            "  354224848179261915075 573147844013817084101 927372692193078999176]\n",
            " [1779979416004714189 2880067194370816120 4660046610375530309\n",
            "  7540113804746346429 12200160415121876738 19740274219868223167\n",
            "  31940434634990099905 51680708854858323072 83621143489848422977\n",
            "  135301852344706746049 218922995834555169026 354224848179261915075\n",
            "  573147844013817084101 927372692193078999176 1500520536206896083277]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[354224848179261915075 573147844013817084101 927372692193078999176\n",
            " 1500520536206896083277 2427893228399975082453]\n",
            "\n",
            "index  26925748508234281076009\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[2880067194370816120 4660046610375530309 7540113804746346429\n",
            "  12200160415121876738 19740274219868223167 31940434634990099905\n",
            "  51680708854858323072 83621143489848422977 135301852344706746049\n",
            "  218922995834555169026 354224848179261915075 573147844013817084101\n",
            "  927372692193078999176 1500520536206896083277 2427893228399975082453]\n",
            " [4660046610375530309 7540113804746346429 12200160415121876738\n",
            "  19740274219868223167 31940434634990099905 51680708854858323072\n",
            "  83621143489848422977 135301852344706746049 218922995834555169026\n",
            "  354224848179261915075 573147844013817084101 927372692193078999176\n",
            "  1500520536206896083277 2427893228399975082453 3928413764606871165730]\n",
            " [7540113804746346429 12200160415121876738 19740274219868223167\n",
            "  31940434634990099905 51680708854858323072 83621143489848422977\n",
            "  135301852344706746049 218922995834555169026 354224848179261915075\n",
            "  573147844013817084101 927372692193078999176 1500520536206896083277\n",
            "  2427893228399975082453 3928413764606871165730 6356306993006846248183]\n",
            " [12200160415121876738 19740274219868223167 31940434634990099905\n",
            "  51680708854858323072 83621143489848422977 135301852344706746049\n",
            "  218922995834555169026 354224848179261915075 573147844013817084101\n",
            "  927372692193078999176 1500520536206896083277 2427893228399975082453\n",
            "  3928413764606871165730 6356306993006846248183 10284720757613717413913]\n",
            " [19740274219868223167 31940434634990099905 51680708854858323072\n",
            "  83621143489848422977 135301852344706746049 218922995834555169026\n",
            "  354224848179261915075 573147844013817084101 927372692193078999176\n",
            "  1500520536206896083277 2427893228399975082453 3928413764606871165730\n",
            "  6356306993006846248183 10284720757613717413913 16641027750620563662096]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[3928413764606871165730 6356306993006846248183 10284720757613717413913\n",
            " 16641027750620563662096 26925748508234281076009]\n",
            "\n",
            "index  298611126818977066918552\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[31940434634990099905 51680708854858323072 83621143489848422977\n",
            "  135301852344706746049 218922995834555169026 354224848179261915075\n",
            "  573147844013817084101 927372692193078999176 1500520536206896083277\n",
            "  2427893228399975082453 3928413764606871165730 6356306993006846248183\n",
            "  10284720757613717413913 16641027750620563662096 26925748508234281076009]\n",
            " [51680708854858323072 83621143489848422977 135301852344706746049\n",
            "  218922995834555169026 354224848179261915075 573147844013817084101\n",
            "  927372692193078999176 1500520536206896083277 2427893228399975082453\n",
            "  3928413764606871165730 6356306993006846248183 10284720757613717413913\n",
            "  16641027750620563662096 26925748508234281076009 43566776258854844738105]\n",
            " [83621143489848422977 135301852344706746049 218922995834555169026\n",
            "  354224848179261915075 573147844013817084101 927372692193078999176\n",
            "  1500520536206896083277 2427893228399975082453 3928413764606871165730\n",
            "  6356306993006846248183 10284720757613717413913 16641027750620563662096\n",
            "  26925748508234281076009 43566776258854844738105 70492524767089125814114]\n",
            " [135301852344706746049 218922995834555169026 354224848179261915075\n",
            "  573147844013817084101 927372692193078999176 1500520536206896083277\n",
            "  2427893228399975082453 3928413764606871165730 6356306993006846248183\n",
            "  10284720757613717413913 16641027750620563662096 26925748508234281076009\n",
            "  43566776258854844738105 70492524767089125814114\n",
            "  114059301025943970552219]\n",
            " [218922995834555169026 354224848179261915075 573147844013817084101\n",
            "  927372692193078999176 1500520536206896083277 2427893228399975082453\n",
            "  3928413764606871165730 6356306993006846248183 10284720757613717413913\n",
            "  16641027750620563662096 26925748508234281076009 43566776258854844738105\n",
            "  70492524767089125814114 114059301025943970552219\n",
            "  184551825793033096366333]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[43566776258854844738105 70492524767089125814114 114059301025943970552219\n",
            " 184551825793033096366333 298611126818977066918552]\n",
            "\n",
            "index  3311648143516982017180081\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[354224848179261915075 573147844013817084101 927372692193078999176\n",
            "  1500520536206896083277 2427893228399975082453 3928413764606871165730\n",
            "  6356306993006846248183 10284720757613717413913 16641027750620563662096\n",
            "  26925748508234281076009 43566776258854844738105 70492524767089125814114\n",
            "  114059301025943970552219 184551825793033096366333\n",
            "  298611126818977066918552]\n",
            " [573147844013817084101 927372692193078999176 1500520536206896083277\n",
            "  2427893228399975082453 3928413764606871165730 6356306993006846248183\n",
            "  10284720757613717413913 16641027750620563662096 26925748508234281076009\n",
            "  43566776258854844738105 70492524767089125814114\n",
            "  114059301025943970552219 184551825793033096366333\n",
            "  298611126818977066918552 483162952612010163284885]\n",
            " [927372692193078999176 1500520536206896083277 2427893228399975082453\n",
            "  3928413764606871165730 6356306993006846248183 10284720757613717413913\n",
            "  16641027750620563662096 26925748508234281076009 43566776258854844738105\n",
            "  70492524767089125814114 114059301025943970552219\n",
            "  184551825793033096366333 298611126818977066918552\n",
            "  483162952612010163284885 781774079430987230203437]\n",
            " [1500520536206896083277 2427893228399975082453 3928413764606871165730\n",
            "  6356306993006846248183 10284720757613717413913 16641027750620563662096\n",
            "  26925748508234281076009 43566776258854844738105 70492524767089125814114\n",
            "  114059301025943970552219 184551825793033096366333\n",
            "  298611126818977066918552 483162952612010163284885\n",
            "  781774079430987230203437 1264937032042997393488322]\n",
            " [2427893228399975082453 3928413764606871165730 6356306993006846248183\n",
            "  10284720757613717413913 16641027750620563662096 26925748508234281076009\n",
            "  43566776258854844738105 70492524767089125814114\n",
            "  114059301025943970552219 184551825793033096366333\n",
            "  298611126818977066918552 483162952612010163284885\n",
            "  781774079430987230203437 1264937032042997393488322\n",
            "  2046711111473984623691759]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[483162952612010163284885 781774079430987230203437\n",
            " 1264937032042997393488322 2046711111473984623691759\n",
            " 3311648143516982017180081]\n",
            "\n",
            "index  36726740705505779255899443\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[3928413764606871165730 6356306993006846248183 10284720757613717413913\n",
            "  16641027750620563662096 26925748508234281076009 43566776258854844738105\n",
            "  70492524767089125814114 114059301025943970552219\n",
            "  184551825793033096366333 298611126818977066918552\n",
            "  483162952612010163284885 781774079430987230203437\n",
            "  1264937032042997393488322 2046711111473984623691759\n",
            "  3311648143516982017180081]\n",
            " [6356306993006846248183 10284720757613717413913 16641027750620563662096\n",
            "  26925748508234281076009 43566776258854844738105 70492524767089125814114\n",
            "  114059301025943970552219 184551825793033096366333\n",
            "  298611126818977066918552 483162952612010163284885\n",
            "  781774079430987230203437 1264937032042997393488322\n",
            "  2046711111473984623691759 3311648143516982017180081\n",
            "  5358359254990966640871840]\n",
            " [10284720757613717413913 16641027750620563662096 26925748508234281076009\n",
            "  43566776258854844738105 70492524767089125814114\n",
            "  114059301025943970552219 184551825793033096366333\n",
            "  298611126818977066918552 483162952612010163284885\n",
            "  781774079430987230203437 1264937032042997393488322\n",
            "  2046711111473984623691759 3311648143516982017180081\n",
            "  5358359254990966640871840 8670007398507948658051921]\n",
            " [16641027750620563662096 26925748508234281076009 43566776258854844738105\n",
            "  70492524767089125814114 114059301025943970552219\n",
            "  184551825793033096366333 298611126818977066918552\n",
            "  483162952612010163284885 781774079430987230203437\n",
            "  1264937032042997393488322 2046711111473984623691759\n",
            "  3311648143516982017180081 5358359254990966640871840\n",
            "  8670007398507948658051921 14028366653498915298923761]\n",
            " [26925748508234281076009 43566776258854844738105 70492524767089125814114\n",
            "  114059301025943970552219 184551825793033096366333\n",
            "  298611126818977066918552 483162952612010163284885\n",
            "  781774079430987230203437 1264937032042997393488322\n",
            "  2046711111473984623691759 3311648143516982017180081\n",
            "  5358359254990966640871840 8670007398507948658051921\n",
            "  14028366653498915298923761 22698374052006863956975682]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[5358359254990966640871840 8670007398507948658051921\n",
            " 14028366653498915298923761 22698374052006863956975682\n",
            " 36726740705505779255899443]\n",
            "\n",
            "index  407305795904080553832073954\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[43566776258854844738105 70492524767089125814114\n",
            "  114059301025943970552219 184551825793033096366333\n",
            "  298611126818977066918552 483162952612010163284885\n",
            "  781774079430987230203437 1264937032042997393488322\n",
            "  2046711111473984623691759 3311648143516982017180081\n",
            "  5358359254990966640871840 8670007398507948658051921\n",
            "  14028366653498915298923761 22698374052006863956975682\n",
            "  36726740705505779255899443]\n",
            " [70492524767089125814114 114059301025943970552219\n",
            "  184551825793033096366333 298611126818977066918552\n",
            "  483162952612010163284885 781774079430987230203437\n",
            "  1264937032042997393488322 2046711111473984623691759\n",
            "  3311648143516982017180081 5358359254990966640871840\n",
            "  8670007398507948658051921 14028366653498915298923761\n",
            "  22698374052006863956975682 36726740705505779255899443\n",
            "  59425114757512643212875125]\n",
            " [114059301025943970552219 184551825793033096366333\n",
            "  298611126818977066918552 483162952612010163284885\n",
            "  781774079430987230203437 1264937032042997393488322\n",
            "  2046711111473984623691759 3311648143516982017180081\n",
            "  5358359254990966640871840 8670007398507948658051921\n",
            "  14028366653498915298923761 22698374052006863956975682\n",
            "  36726740705505779255899443 59425114757512643212875125\n",
            "  96151855463018422468774568]\n",
            " [184551825793033096366333 298611126818977066918552\n",
            "  483162952612010163284885 781774079430987230203437\n",
            "  1264937032042997393488322 2046711111473984623691759\n",
            "  3311648143516982017180081 5358359254990966640871840\n",
            "  8670007398507948658051921 14028366653498915298923761\n",
            "  22698374052006863956975682 36726740705505779255899443\n",
            "  59425114757512643212875125 96151855463018422468774568\n",
            "  155576970220531065681649693]\n",
            " [298611126818977066918552 483162952612010163284885\n",
            "  781774079430987230203437 1264937032042997393488322\n",
            "  2046711111473984623691759 3311648143516982017180081\n",
            "  5358359254990966640871840 8670007398507948658051921\n",
            "  14028366653498915298923761 22698374052006863956975682\n",
            "  36726740705505779255899443 59425114757512643212875125\n",
            "  96151855463018422468774568 155576970220531065681649693\n",
            "  251728825683549488150424261]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[59425114757512643212875125 96151855463018422468774568\n",
            " 155576970220531065681649693 251728825683549488150424261\n",
            " 407305795904080553832073954]\n",
            "\n",
            "index  4517090495650391871408712937\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[483162952612010163284885 781774079430987230203437\n",
            "  1264937032042997393488322 2046711111473984623691759\n",
            "  3311648143516982017180081 5358359254990966640871840\n",
            "  8670007398507948658051921 14028366653498915298923761\n",
            "  22698374052006863956975682 36726740705505779255899443\n",
            "  59425114757512643212875125 96151855463018422468774568\n",
            "  155576970220531065681649693 251728825683549488150424261\n",
            "  407305795904080553832073954]\n",
            " [781774079430987230203437 1264937032042997393488322\n",
            "  2046711111473984623691759 3311648143516982017180081\n",
            "  5358359254990966640871840 8670007398507948658051921\n",
            "  14028366653498915298923761 22698374052006863956975682\n",
            "  36726740705505779255899443 59425114757512643212875125\n",
            "  96151855463018422468774568 155576970220531065681649693\n",
            "  251728825683549488150424261 407305795904080553832073954\n",
            "  659034621587630041982498215]\n",
            " [1264937032042997393488322 2046711111473984623691759\n",
            "  3311648143516982017180081 5358359254990966640871840\n",
            "  8670007398507948658051921 14028366653498915298923761\n",
            "  22698374052006863956975682 36726740705505779255899443\n",
            "  59425114757512643212875125 96151855463018422468774568\n",
            "  155576970220531065681649693 251728825683549488150424261\n",
            "  407305795904080553832073954 659034621587630041982498215\n",
            "  1066340417491710595814572169]\n",
            " [2046711111473984623691759 3311648143516982017180081\n",
            "  5358359254990966640871840 8670007398507948658051921\n",
            "  14028366653498915298923761 22698374052006863956975682\n",
            "  36726740705505779255899443 59425114757512643212875125\n",
            "  96151855463018422468774568 155576970220531065681649693\n",
            "  251728825683549488150424261 407305795904080553832073954\n",
            "  659034621587630041982498215 1066340417491710595814572169\n",
            "  1725375039079340637797070384]\n",
            " [3311648143516982017180081 5358359254990966640871840\n",
            "  8670007398507948658051921 14028366653498915298923761\n",
            "  22698374052006863956975682 36726740705505779255899443\n",
            "  59425114757512643212875125 96151855463018422468774568\n",
            "  155576970220531065681649693 251728825683549488150424261\n",
            "  407305795904080553832073954 659034621587630041982498215\n",
            "  1066340417491710595814572169 1725375039079340637797070384\n",
            "  2791715456571051233611642553]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[659034621587630041982498215 1066340417491710595814572169\n",
            " 1725375039079340637797070384 2791715456571051233611642553\n",
            " 4517090495650391871408712937]\n",
            "\n",
            "index  50095301248058391139327916261\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[5358359254990966640871840 8670007398507948658051921\n",
            "  14028366653498915298923761 22698374052006863956975682\n",
            "  36726740705505779255899443 59425114757512643212875125\n",
            "  96151855463018422468774568 155576970220531065681649693\n",
            "  251728825683549488150424261 407305795904080553832073954\n",
            "  659034621587630041982498215 1066340417491710595814572169\n",
            "  1725375039079340637797070384 2791715456571051233611642553\n",
            "  4517090495650391871408712937]\n",
            " [8670007398507948658051921 14028366653498915298923761\n",
            "  22698374052006863956975682 36726740705505779255899443\n",
            "  59425114757512643212875125 96151855463018422468774568\n",
            "  155576970220531065681649693 251728825683549488150424261\n",
            "  407305795904080553832073954 659034621587630041982498215\n",
            "  1066340417491710595814572169 1725375039079340637797070384\n",
            "  2791715456571051233611642553 4517090495650391871408712937\n",
            "  7308805952221443105020355490]\n",
            " [14028366653498915298923761 22698374052006863956975682\n",
            "  36726740705505779255899443 59425114757512643212875125\n",
            "  96151855463018422468774568 155576970220531065681649693\n",
            "  251728825683549488150424261 407305795904080553832073954\n",
            "  659034621587630041982498215 1066340417491710595814572169\n",
            "  1725375039079340637797070384 2791715456571051233611642553\n",
            "  4517090495650391871408712937 7308805952221443105020355490\n",
            "  11825896447871834976429068427]\n",
            " [22698374052006863956975682 36726740705505779255899443\n",
            "  59425114757512643212875125 96151855463018422468774568\n",
            "  155576970220531065681649693 251728825683549488150424261\n",
            "  407305795904080553832073954 659034621587630041982498215\n",
            "  1066340417491710595814572169 1725375039079340637797070384\n",
            "  2791715456571051233611642553 4517090495650391871408712937\n",
            "  7308805952221443105020355490 11825896447871834976429068427\n",
            "  19134702400093278081449423917]\n",
            " [36726740705505779255899443 59425114757512643212875125\n",
            "  96151855463018422468774568 155576970220531065681649693\n",
            "  251728825683549488150424261 407305795904080553832073954\n",
            "  659034621587630041982498215 1066340417491710595814572169\n",
            "  1725375039079340637797070384 2791715456571051233611642553\n",
            "  4517090495650391871408712937 7308805952221443105020355490\n",
            "  11825896447871834976429068427 19134702400093278081449423917\n",
            "  30960598847965113057878492344]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[7308805952221443105020355490 11825896447871834976429068427\n",
            " 19134702400093278081449423917 30960598847965113057878492344\n",
            " 50095301248058391139327916261]\n",
            "\n",
            "index  555565404224292694404015791808\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[59425114757512643212875125 96151855463018422468774568\n",
            "  155576970220531065681649693 251728825683549488150424261\n",
            "  407305795904080553832073954 659034621587630041982498215\n",
            "  1066340417491710595814572169 1725375039079340637797070384\n",
            "  2791715456571051233611642553 4517090495650391871408712937\n",
            "  7308805952221443105020355490 11825896447871834976429068427\n",
            "  19134702400093278081449423917 30960598847965113057878492344\n",
            "  50095301248058391139327916261]\n",
            " [96151855463018422468774568 155576970220531065681649693\n",
            "  251728825683549488150424261 407305795904080553832073954\n",
            "  659034621587630041982498215 1066340417491710595814572169\n",
            "  1725375039079340637797070384 2791715456571051233611642553\n",
            "  4517090495650391871408712937 7308805952221443105020355490\n",
            "  11825896447871834976429068427 19134702400093278081449423917\n",
            "  30960598847965113057878492344 50095301248058391139327916261\n",
            "  81055900096023504197206408605]\n",
            " [155576970220531065681649693 251728825683549488150424261\n",
            "  407305795904080553832073954 659034621587630041982498215\n",
            "  1066340417491710595814572169 1725375039079340637797070384\n",
            "  2791715456571051233611642553 4517090495650391871408712937\n",
            "  7308805952221443105020355490 11825896447871834976429068427\n",
            "  19134702400093278081449423917 30960598847965113057878492344\n",
            "  50095301248058391139327916261 81055900096023504197206408605\n",
            "  131151201344081895336534324866]\n",
            " [251728825683549488150424261 407305795904080553832073954\n",
            "  659034621587630041982498215 1066340417491710595814572169\n",
            "  1725375039079340637797070384 2791715456571051233611642553\n",
            "  4517090495650391871408712937 7308805952221443105020355490\n",
            "  11825896447871834976429068427 19134702400093278081449423917\n",
            "  30960598847965113057878492344 50095301248058391139327916261\n",
            "  81055900096023504197206408605 131151201344081895336534324866\n",
            "  212207101440105399533740733471]\n",
            " [407305795904080553832073954 659034621587630041982498215\n",
            "  1066340417491710595814572169 1725375039079340637797070384\n",
            "  2791715456571051233611642553 4517090495650391871408712937\n",
            "  7308805952221443105020355490 11825896447871834976429068427\n",
            "  19134702400093278081449423917 30960598847965113057878492344\n",
            "  50095301248058391139327916261 81055900096023504197206408605\n",
            "  131151201344081895336534324866 212207101440105399533740733471\n",
            "  343358302784187294870275058337]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[81055900096023504197206408605 131151201344081895336534324866\n",
            " 212207101440105399533740733471 343358302784187294870275058337\n",
            " 555565404224292694404015791808]\n",
            "\n",
            "index  6161314747715278029583501626149\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[659034621587630041982498215 1066340417491710595814572169\n",
            "  1725375039079340637797070384 2791715456571051233611642553\n",
            "  4517090495650391871408712937 7308805952221443105020355490\n",
            "  11825896447871834976429068427 19134702400093278081449423917\n",
            "  30960598847965113057878492344 50095301248058391139327916261\n",
            "  81055900096023504197206408605 131151201344081895336534324866\n",
            "  212207101440105399533740733471 343358302784187294870275058337\n",
            "  555565404224292694404015791808]\n",
            " [1066340417491710595814572169 1725375039079340637797070384\n",
            "  2791715456571051233611642553 4517090495650391871408712937\n",
            "  7308805952221443105020355490 11825896447871834976429068427\n",
            "  19134702400093278081449423917 30960598847965113057878492344\n",
            "  50095301248058391139327916261 81055900096023504197206408605\n",
            "  131151201344081895336534324866 212207101440105399533740733471\n",
            "  343358302784187294870275058337 555565404224292694404015791808\n",
            "  898923707008479989274290850145]\n",
            " [1725375039079340637797070384 2791715456571051233611642553\n",
            "  4517090495650391871408712937 7308805952221443105020355490\n",
            "  11825896447871834976429068427 19134702400093278081449423917\n",
            "  30960598847965113057878492344 50095301248058391139327916261\n",
            "  81055900096023504197206408605 131151201344081895336534324866\n",
            "  212207101440105399533740733471 343358302784187294870275058337\n",
            "  555565404224292694404015791808 898923707008479989274290850145\n",
            "  1454489111232772683678306641953]\n",
            " [2791715456571051233611642553 4517090495650391871408712937\n",
            "  7308805952221443105020355490 11825896447871834976429068427\n",
            "  19134702400093278081449423917 30960598847965113057878492344\n",
            "  50095301248058391139327916261 81055900096023504197206408605\n",
            "  131151201344081895336534324866 212207101440105399533740733471\n",
            "  343358302784187294870275058337 555565404224292694404015791808\n",
            "  898923707008479989274290850145 1454489111232772683678306641953\n",
            "  2353412818241252672952597492098]\n",
            " [4517090495650391871408712937 7308805952221443105020355490\n",
            "  11825896447871834976429068427 19134702400093278081449423917\n",
            "  30960598847965113057878492344 50095301248058391139327916261\n",
            "  81055900096023504197206408605 131151201344081895336534324866\n",
            "  212207101440105399533740733471 343358302784187294870275058337\n",
            "  555565404224292694404015791808 898923707008479989274290850145\n",
            "  1454489111232772683678306641953 2353412818241252672952597492098\n",
            "  3807901929474025356630904134051]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[898923707008479989274290850145 1454489111232772683678306641953\n",
            " 2353412818241252672952597492098 3807901929474025356630904134051\n",
            " 6161314747715278029583501626149]\n",
            "\n",
            "index  68330027629092351019822533679447\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[7308805952221443105020355490 11825896447871834976429068427\n",
            "  19134702400093278081449423917 30960598847965113057878492344\n",
            "  50095301248058391139327916261 81055900096023504197206408605\n",
            "  131151201344081895336534324866 212207101440105399533740733471\n",
            "  343358302784187294870275058337 555565404224292694404015791808\n",
            "  898923707008479989274290850145 1454489111232772683678306641953\n",
            "  2353412818241252672952597492098 3807901929474025356630904134051\n",
            "  6161314747715278029583501626149]\n",
            " [11825896447871834976429068427 19134702400093278081449423917\n",
            "  30960598847965113057878492344 50095301248058391139327916261\n",
            "  81055900096023504197206408605 131151201344081895336534324866\n",
            "  212207101440105399533740733471 343358302784187294870275058337\n",
            "  555565404224292694404015791808 898923707008479989274290850145\n",
            "  1454489111232772683678306641953 2353412818241252672952597492098\n",
            "  3807901929474025356630904134051 6161314747715278029583501626149\n",
            "  9969216677189303386214405760200]\n",
            " [19134702400093278081449423917 30960598847965113057878492344\n",
            "  50095301248058391139327916261 81055900096023504197206408605\n",
            "  131151201344081895336534324866 212207101440105399533740733471\n",
            "  343358302784187294870275058337 555565404224292694404015791808\n",
            "  898923707008479989274290850145 1454489111232772683678306641953\n",
            "  2353412818241252672952597492098 3807901929474025356630904134051\n",
            "  6161314747715278029583501626149 9969216677189303386214405760200\n",
            "  16130531424904581415797907386349]\n",
            " [30960598847965113057878492344 50095301248058391139327916261\n",
            "  81055900096023504197206408605 131151201344081895336534324866\n",
            "  212207101440105399533740733471 343358302784187294870275058337\n",
            "  555565404224292694404015791808 898923707008479989274290850145\n",
            "  1454489111232772683678306641953 2353412818241252672952597492098\n",
            "  3807901929474025356630904134051 6161314747715278029583501626149\n",
            "  9969216677189303386214405760200 16130531424904581415797907386349\n",
            "  26099748102093884802012313146549]\n",
            " [50095301248058391139327916261 81055900096023504197206408605\n",
            "  131151201344081895336534324866 212207101440105399533740733471\n",
            "  343358302784187294870275058337 555565404224292694404015791808\n",
            "  898923707008479989274290850145 1454489111232772683678306641953\n",
            "  2353412818241252672952597492098 3807901929474025356630904134051\n",
            "  6161314747715278029583501626149 9969216677189303386214405760200\n",
            "  16130531424904581415797907386349 26099748102093884802012313146549\n",
            "  42230279526998466217810220532898]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[9969216677189303386214405760200 16130531424904581415797907386349\n",
            " 26099748102093884802012313146549 42230279526998466217810220532898\n",
            " 68330027629092351019822533679447]\n",
            "\n",
            "index  757791618667731139247631372100066\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[81055900096023504197206408605 131151201344081895336534324866\n",
            "  212207101440105399533740733471 343358302784187294870275058337\n",
            "  555565404224292694404015791808 898923707008479989274290850145\n",
            "  1454489111232772683678306641953 2353412818241252672952597492098\n",
            "  3807901929474025356630904134051 6161314747715278029583501626149\n",
            "  9969216677189303386214405760200 16130531424904581415797907386349\n",
            "  26099748102093884802012313146549 42230279526998466217810220532898\n",
            "  68330027629092351019822533679447]\n",
            " [131151201344081895336534324866 212207101440105399533740733471\n",
            "  343358302784187294870275058337 555565404224292694404015791808\n",
            "  898923707008479989274290850145 1454489111232772683678306641953\n",
            "  2353412818241252672952597492098 3807901929474025356630904134051\n",
            "  6161314747715278029583501626149 9969216677189303386214405760200\n",
            "  16130531424904581415797907386349 26099748102093884802012313146549\n",
            "  42230279526998466217810220532898 68330027629092351019822533679447\n",
            "  110560307156090817237632754212345]\n",
            " [212207101440105399533740733471 343358302784187294870275058337\n",
            "  555565404224292694404015791808 898923707008479989274290850145\n",
            "  1454489111232772683678306641953 2353412818241252672952597492098\n",
            "  3807901929474025356630904134051 6161314747715278029583501626149\n",
            "  9969216677189303386214405760200 16130531424904581415797907386349\n",
            "  26099748102093884802012313146549 42230279526998466217810220532898\n",
            "  68330027629092351019822533679447 110560307156090817237632754212345\n",
            "  178890334785183168257455287891792]\n",
            " [343358302784187294870275058337 555565404224292694404015791808\n",
            "  898923707008479989274290850145 1454489111232772683678306641953\n",
            "  2353412818241252672952597492098 3807901929474025356630904134051\n",
            "  6161314747715278029583501626149 9969216677189303386214405760200\n",
            "  16130531424904581415797907386349 26099748102093884802012313146549\n",
            "  42230279526998466217810220532898 68330027629092351019822533679447\n",
            "  110560307156090817237632754212345 178890334785183168257455287891792\n",
            "  289450641941273985495088042104137]\n",
            " [555565404224292694404015791808 898923707008479989274290850145\n",
            "  1454489111232772683678306641953 2353412818241252672952597492098\n",
            "  3807901929474025356630904134051 6161314747715278029583501626149\n",
            "  9969216677189303386214405760200 16130531424904581415797907386349\n",
            "  26099748102093884802012313146549 42230279526998466217810220532898\n",
            "  68330027629092351019822533679447 110560307156090817237632754212345\n",
            "  178890334785183168257455287891792 289450641941273985495088042104137\n",
            "  468340976726457153752543329995929]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[110560307156090817237632754212345 178890334785183168257455287891792\n",
            " 289450641941273985495088042104137 468340976726457153752543329995929\n",
            " 757791618667731139247631372100066]\n",
            "\n",
            "index  11463113765491467695340528626429782121\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[1226132595394188293000174702095995 1983924214061919432247806074196061\n",
            "  3210056809456107725247980776292056 5193981023518027157495786850488117\n",
            "  8404037832974134882743767626780173 13598018856492162040239554477268290\n",
            "  22002056689466296922983322104048463 35600075545958458963222876581316753\n",
            "  57602132235424755886206198685365216 93202207781383214849429075266681969\n",
            "  150804340016807970735635273952047185\n",
            "  244006547798191185585064349218729154\n",
            "  394810887814999156320699623170776339\n",
            "  638817435613190341905763972389505493\n",
            "  1033628323428189498226463595560281832]\n",
            " [1983924214061919432247806074196061 3210056809456107725247980776292056\n",
            "  5193981023518027157495786850488117 8404037832974134882743767626780173\n",
            "  13598018856492162040239554477268290 22002056689466296922983322104048463\n",
            "  35600075545958458963222876581316753 57602132235424755886206198685365216\n",
            "  93202207781383214849429075266681969\n",
            "  150804340016807970735635273952047185\n",
            "  244006547798191185585064349218729154\n",
            "  394810887814999156320699623170776339\n",
            "  638817435613190341905763972389505493\n",
            "  1033628323428189498226463595560281832\n",
            "  1672445759041379840132227567949787325]\n",
            " [3210056809456107725247980776292056 5193981023518027157495786850488117\n",
            "  8404037832974134882743767626780173 13598018856492162040239554477268290\n",
            "  22002056689466296922983322104048463 35600075545958458963222876581316753\n",
            "  57602132235424755886206198685365216 93202207781383214849429075266681969\n",
            "  150804340016807970735635273952047185\n",
            "  244006547798191185585064349218729154\n",
            "  394810887814999156320699623170776339\n",
            "  638817435613190341905763972389505493\n",
            "  1033628323428189498226463595560281832\n",
            "  1672445759041379840132227567949787325\n",
            "  2706074082469569338358691163510069157]\n",
            " [5193981023518027157495786850488117 8404037832974134882743767626780173\n",
            "  13598018856492162040239554477268290 22002056689466296922983322104048463\n",
            "  35600075545958458963222876581316753 57602132235424755886206198685365216\n",
            "  93202207781383214849429075266681969\n",
            "  150804340016807970735635273952047185\n",
            "  244006547798191185585064349218729154\n",
            "  394810887814999156320699623170776339\n",
            "  638817435613190341905763972389505493\n",
            "  1033628323428189498226463595560281832\n",
            "  1672445759041379840132227567949787325\n",
            "  2706074082469569338358691163510069157\n",
            "  4378519841510949178490918731459856482]\n",
            " [8404037832974134882743767626780173 13598018856492162040239554477268290\n",
            "  22002056689466296922983322104048463 35600075545958458963222876581316753\n",
            "  57602132235424755886206198685365216 93202207781383214849429075266681969\n",
            "  150804340016807970735635273952047185\n",
            "  244006547798191185585064349218729154\n",
            "  394810887814999156320699623170776339\n",
            "  638817435613190341905763972389505493\n",
            "  1033628323428189498226463595560281832\n",
            "  1672445759041379840132227567949787325\n",
            "  2706074082469569338358691163510069157\n",
            "  4378519841510949178490918731459856482\n",
            "  7084593923980518516849609894969925639]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[1672445759041379840132227567949787325\n",
            " 2706074082469569338358691163510069157\n",
            " 4378519841510949178490918731459856482\n",
            " 7084593923980518516849609894969925639\n",
            " 11463113765491467695340528626429782121]\n",
            "\n",
            "index  173402521172797813159685037284371942044301\n",
            "    x: <class 'numpy.ndarray'> (5, 15) \n",
            "[[18547707689471986212190138521399707760\n",
            "  30010821454963453907530667147829489881\n",
            "  48558529144435440119720805669229197641\n",
            "  78569350599398894027251472817058687522\n",
            "  127127879743834334146972278486287885163\n",
            "  205697230343233228174223751303346572685\n",
            "  332825110087067562321196029789634457848\n",
            "  538522340430300790495419781092981030533\n",
            "  871347450517368352816615810882615488381\n",
            "  1409869790947669143312035591975596518914\n",
            "  2281217241465037496128651402858212007295\n",
            "  3691087032412706639440686994833808526209\n",
            "  5972304273877744135569338397692020533504\n",
            "  9663391306290450775010025392525829059713\n",
            "  15635695580168194910579363790217849593217]\n",
            " [30010821454963453907530667147829489881\n",
            "  48558529144435440119720805669229197641\n",
            "  78569350599398894027251472817058687522\n",
            "  127127879743834334146972278486287885163\n",
            "  205697230343233228174223751303346572685\n",
            "  332825110087067562321196029789634457848\n",
            "  538522340430300790495419781092981030533\n",
            "  871347450517368352816615810882615488381\n",
            "  1409869790947669143312035591975596518914\n",
            "  2281217241465037496128651402858212007295\n",
            "  3691087032412706639440686994833808526209\n",
            "  5972304273877744135569338397692020533504\n",
            "  9663391306290450775010025392525829059713\n",
            "  15635695580168194910579363790217849593217\n",
            "  25299086886458645685589389182743678652930]\n",
            " [48558529144435440119720805669229197641\n",
            "  78569350599398894027251472817058687522\n",
            "  127127879743834334146972278486287885163\n",
            "  205697230343233228174223751303346572685\n",
            "  332825110087067562321196029789634457848\n",
            "  538522340430300790495419781092981030533\n",
            "  871347450517368352816615810882615488381\n",
            "  1409869790947669143312035591975596518914\n",
            "  2281217241465037496128651402858212007295\n",
            "  3691087032412706639440686994833808526209\n",
            "  5972304273877744135569338397692020533504\n",
            "  9663391306290450775010025392525829059713\n",
            "  15635695580168194910579363790217849593217\n",
            "  25299086886458645685589389182743678652930\n",
            "  40934782466626840596168752972961528246147]\n",
            " [78569350599398894027251472817058687522\n",
            "  127127879743834334146972278486287885163\n",
            "  205697230343233228174223751303346572685\n",
            "  332825110087067562321196029789634457848\n",
            "  538522340430300790495419781092981030533\n",
            "  871347450517368352816615810882615488381\n",
            "  1409869790947669143312035591975596518914\n",
            "  2281217241465037496128651402858212007295\n",
            "  3691087032412706639440686994833808526209\n",
            "  5972304273877744135569338397692020533504\n",
            "  9663391306290450775010025392525829059713\n",
            "  15635695580168194910579363790217849593217\n",
            "  25299086886458645685589389182743678652930\n",
            "  40934782466626840596168752972961528246147\n",
            "  66233869353085486281758142155705206899077]\n",
            " [127127879743834334146972278486287885163\n",
            "  205697230343233228174223751303346572685\n",
            "  332825110087067562321196029789634457848\n",
            "  538522340430300790495419781092981030533\n",
            "  871347450517368352816615810882615488381\n",
            "  1409869790947669143312035591975596518914\n",
            "  2281217241465037496128651402858212007295\n",
            "  3691087032412706639440686994833808526209\n",
            "  5972304273877744135569338397692020533504\n",
            "  9663391306290450775010025392525829059713\n",
            "  15635695580168194910579363790217849593217\n",
            "  25299086886458645685589389182743678652930\n",
            "  40934782466626840596168752972961528246147\n",
            "  66233869353085486281758142155705206899077\n",
            "  107168651819712326877926895128666735145224]]\n",
            "    y: <class 'numpy.ndarray'> (5,) \n",
            "[25299086886458645685589389182743678652930\n",
            " 40934782466626840596168752972961528246147\n",
            " 66233869353085486281758142155705206899077\n",
            " 107168651819712326877926895128666735145224\n",
            " 173402521172797813159685037284371942044301]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyuNumpsdJ2v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "76ad880b-f93f-4dfe-9ea7-c2cfa7bc04d8"
      },
      "source": [
        "print(\"\\n=== Train data ===\")\n",
        "print( \"len(gen_train) =\", len( gen_train ) )\n",
        "x_train , y_train = np.array(list_x_train), np.array(list_y_train)\n",
        "# print( \"Shape of x_train and y_train:\" , x_train.shape, y_train.shape )\n",
        "print( \"Preview the first five rows:\" )\n",
        "for i in range(5):\n",
        "    print( \"    \", x_train[i] , y_train[i] )    \n",
        "\n",
        "print(\"\\n=== Test data ===\")\n",
        "print( \"len(gen_test) =\", len( gen_test ) )\n",
        "x_test , y_test = np.array(list_x_test), np.array(list_y_test)\n",
        "# print( \"Shape of x_test and y_test:\" , x_test.shape, y_test.shape )\n",
        "print( \"Preview the first five rows:\" )\n",
        "for i in range(5):\n",
        "    print( \"    \", x_test[i] , y_test[i] )    \n",
        "\n",
        "print(\"\\n=== Validation data ===\")\n",
        "print( \"len(gen_test) =\", len( gen_val ) )\n",
        "x_val , y_val = np.array(list_x_val), np.array(list_y_val)\n",
        "# print( \"Shape of x_val and y_val:\" , x_val.shape, y_val.shape )\n",
        "print( \"Preview the first five rows:\" )\n",
        "for i in range(5):\n",
        "    print( \"    \", x_val[i] , y_val[i] )   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=== Train data ===\n",
            "len(gen_train) = 29\n",
            "Preview the first five rows:\n",
            "     [0 1 1 2 3 5 8 13 21 34 55 89 144 233 377] 610\n",
            "     [1 1 2 3 5 8 13 21 34 55 89 144 233 377 610] 987\n",
            "     [1 2 3 5 8 13 21 34 55 89 144 233 377 610 987] 1597\n",
            "     [2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597] 2584\n",
            "     [3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584] 4181\n",
            "\n",
            "=== Test data ===\n",
            "len(gen_test) = 1\n",
            "Preview the first five rows:\n",
            "     [1226132595394188293000174702095995 1983924214061919432247806074196061\n",
            " 3210056809456107725247980776292056 5193981023518027157495786850488117\n",
            " 8404037832974134882743767626780173 13598018856492162040239554477268290\n",
            " 22002056689466296922983322104048463 35600075545958458963222876581316753\n",
            " 57602132235424755886206198685365216 93202207781383214849429075266681969\n",
            " 150804340016807970735635273952047185 244006547798191185585064349218729154\n",
            " 394810887814999156320699623170776339 638817435613190341905763972389505493\n",
            " 1033628323428189498226463595560281832] 1672445759041379840132227567949787325\n",
            "     [1983924214061919432247806074196061 3210056809456107725247980776292056\n",
            " 5193981023518027157495786850488117 8404037832974134882743767626780173\n",
            " 13598018856492162040239554477268290 22002056689466296922983322104048463\n",
            " 35600075545958458963222876581316753 57602132235424755886206198685365216\n",
            " 93202207781383214849429075266681969 150804340016807970735635273952047185\n",
            " 244006547798191185585064349218729154 394810887814999156320699623170776339\n",
            " 638817435613190341905763972389505493\n",
            " 1033628323428189498226463595560281832\n",
            " 1672445759041379840132227567949787325] 2706074082469569338358691163510069157\n",
            "     [3210056809456107725247980776292056 5193981023518027157495786850488117\n",
            " 8404037832974134882743767626780173 13598018856492162040239554477268290\n",
            " 22002056689466296922983322104048463 35600075545958458963222876581316753\n",
            " 57602132235424755886206198685365216 93202207781383214849429075266681969\n",
            " 150804340016807970735635273952047185 244006547798191185585064349218729154\n",
            " 394810887814999156320699623170776339 638817435613190341905763972389505493\n",
            " 1033628323428189498226463595560281832\n",
            " 1672445759041379840132227567949787325\n",
            " 2706074082469569338358691163510069157] 4378519841510949178490918731459856482\n",
            "     [5193981023518027157495786850488117 8404037832974134882743767626780173\n",
            " 13598018856492162040239554477268290 22002056689466296922983322104048463\n",
            " 35600075545958458963222876581316753 57602132235424755886206198685365216\n",
            " 93202207781383214849429075266681969 150804340016807970735635273952047185\n",
            " 244006547798191185585064349218729154 394810887814999156320699623170776339\n",
            " 638817435613190341905763972389505493\n",
            " 1033628323428189498226463595560281832\n",
            " 1672445759041379840132227567949787325\n",
            " 2706074082469569338358691163510069157\n",
            " 4378519841510949178490918731459856482] 7084593923980518516849609894969925639\n",
            "     [8404037832974134882743767626780173 13598018856492162040239554477268290\n",
            " 22002056689466296922983322104048463 35600075545958458963222876581316753\n",
            " 57602132235424755886206198685365216 93202207781383214849429075266681969\n",
            " 150804340016807970735635273952047185 244006547798191185585064349218729154\n",
            " 394810887814999156320699623170776339 638817435613190341905763972389505493\n",
            " 1033628323428189498226463595560281832\n",
            " 1672445759041379840132227567949787325\n",
            " 2706074082469569338358691163510069157\n",
            " 4378519841510949178490918731459856482\n",
            " 7084593923980518516849609894969925639] 11463113765491467695340528626429782121\n",
            "\n",
            "=== Validation data ===\n",
            "len(gen_test) = 1\n",
            "Preview the first five rows:\n",
            "     [18547707689471986212190138521399707760\n",
            " 30010821454963453907530667147829489881\n",
            " 48558529144435440119720805669229197641\n",
            " 78569350599398894027251472817058687522\n",
            " 127127879743834334146972278486287885163\n",
            " 205697230343233228174223751303346572685\n",
            " 332825110087067562321196029789634457848\n",
            " 538522340430300790495419781092981030533\n",
            " 871347450517368352816615810882615488381\n",
            " 1409869790947669143312035591975596518914\n",
            " 2281217241465037496128651402858212007295\n",
            " 3691087032412706639440686994833808526209\n",
            " 5972304273877744135569338397692020533504\n",
            " 9663391306290450775010025392525829059713\n",
            " 15635695580168194910579363790217849593217] 25299086886458645685589389182743678652930\n",
            "     [30010821454963453907530667147829489881\n",
            " 48558529144435440119720805669229197641\n",
            " 78569350599398894027251472817058687522\n",
            " 127127879743834334146972278486287885163\n",
            " 205697230343233228174223751303346572685\n",
            " 332825110087067562321196029789634457848\n",
            " 538522340430300790495419781092981030533\n",
            " 871347450517368352816615810882615488381\n",
            " 1409869790947669143312035591975596518914\n",
            " 2281217241465037496128651402858212007295\n",
            " 3691087032412706639440686994833808526209\n",
            " 5972304273877744135569338397692020533504\n",
            " 9663391306290450775010025392525829059713\n",
            " 15635695580168194910579363790217849593217\n",
            " 25299086886458645685589389182743678652930] 40934782466626840596168752972961528246147\n",
            "     [48558529144435440119720805669229197641\n",
            " 78569350599398894027251472817058687522\n",
            " 127127879743834334146972278486287885163\n",
            " 205697230343233228174223751303346572685\n",
            " 332825110087067562321196029789634457848\n",
            " 538522340430300790495419781092981030533\n",
            " 871347450517368352816615810882615488381\n",
            " 1409869790947669143312035591975596518914\n",
            " 2281217241465037496128651402858212007295\n",
            " 3691087032412706639440686994833808526209\n",
            " 5972304273877744135569338397692020533504\n",
            " 9663391306290450775010025392525829059713\n",
            " 15635695580168194910579363790217849593217\n",
            " 25299086886458645685589389182743678652930\n",
            " 40934782466626840596168752972961528246147] 66233869353085486281758142155705206899077\n",
            "     [78569350599398894027251472817058687522\n",
            " 127127879743834334146972278486287885163\n",
            " 205697230343233228174223751303346572685\n",
            " 332825110087067562321196029789634457848\n",
            " 538522340430300790495419781092981030533\n",
            " 871347450517368352816615810882615488381\n",
            " 1409869790947669143312035591975596518914\n",
            " 2281217241465037496128651402858212007295\n",
            " 3691087032412706639440686994833808526209\n",
            " 5972304273877744135569338397692020533504\n",
            " 9663391306290450775010025392525829059713\n",
            " 15635695580168194910579363790217849593217\n",
            " 25299086886458645685589389182743678652930\n",
            " 40934782466626840596168752972961528246147\n",
            " 66233869353085486281758142155705206899077] 107168651819712326877926895128666735145224\n",
            "     [127127879743834334146972278486287885163\n",
            " 205697230343233228174223751303346572685\n",
            " 332825110087067562321196029789634457848\n",
            " 538522340430300790495419781092981030533\n",
            " 871347450517368352816615810882615488381\n",
            " 1409869790947669143312035591975596518914\n",
            " 2281217241465037496128651402858212007295\n",
            " 3691087032412706639440686994833808526209\n",
            " 5972304273877744135569338397692020533504\n",
            " 9663391306290450775010025392525829059713\n",
            " 15635695580168194910579363790217849593217\n",
            " 25299086886458645685589389182743678652930\n",
            " 40934782466626840596168752972961528246147\n",
            " 66233869353085486281758142155705206899077\n",
            " 107168651819712326877926895128666735145224] 173402521172797813159685037284371942044301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEethAfGeTpP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "f0582699-26f9-482e-cb48-79cd92c45abc"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, ..., 144, 233, 377],\n",
              "       [1, 1, 2, ..., 233, 377, 610],\n",
              "       [1, 2, 3, ..., 377, 610, 987],\n",
              "       ...,\n",
              "       [212207101440105399533740733471, 343358302784187294870275058337,\n",
              "        555565404224292694404015791808, ...,\n",
              "        68330027629092351019822533679447,\n",
              "        110560307156090817237632754212345,\n",
              "        178890334785183168257455287891792],\n",
              "       [343358302784187294870275058337, 555565404224292694404015791808,\n",
              "        898923707008479989274290850145, ...,\n",
              "        110560307156090817237632754212345,\n",
              "        178890334785183168257455287891792,\n",
              "        289450641941273985495088042104137],\n",
              "       [555565404224292694404015791808, 898923707008479989274290850145,\n",
              "        1454489111232772683678306641953, ...,\n",
              "        178890334785183168257455287891792,\n",
              "        289450641941273985495088042104137,\n",
              "        468340976726457153752543329995929]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQM2IIikKfG9"
      },
      "source": [
        "# # Create three arrays (each with time_steps=10, output_dim=3)\n",
        "# time_steps = x_train.shape[1]\n",
        "# y_train_temp = np.zeros( ( y_train.shape[0], time_steps, 3) , dtype=np.float32 )\n",
        "# y_test_temp = np.zeros( ( y_test.shape[0], time_steps, 3) , dtype=np.float32 )\n",
        "# y_val_temp = np.zeros( ( y_val.shape[0], time_steps, 3) , dtype=np.float32 )\n",
        "\n",
        "# # Fill the three new arrays with the first predicted numbers (for the last time step only) \n",
        "# y_train_temp[ : , -1, 0 ] = y_train[ : ]\n",
        "# y_test_temp[ : , -1, 0 ] = y_test[ : ]\n",
        "# y_val_temp[ : , -1, 0 ] = y_val[ : ]\n",
        "\n",
        "# # For the last time step only, compute values for the second and third predicted numbers\n",
        "# y_train_temp[ : , -1, 1 ] = y_train_temp[ : , -1, 0 ]\n",
        "# y_train_temp[ : , -1, 2 ] = y_train_temp[ : , -1, 0 ]\n",
        "# y_test_temp[ : , -1, 1 ] = y_test_temp[ : , -1, 0 ]\n",
        "# y_test_temp[ : , -1, 2 ] = y_test_temp[ : , -1, 0 ]\n",
        "# y_val_temp[ : , -1, 1 ] = y_val_temp[ : , -1, 0 ]\n",
        "# y_val_temp[ : , -1, 2 ] = y_val_temp[ : , -1, 0 ]\n",
        "\n",
        "# # Fill vectors of 3 predicted numbers to the remaining time steps\n",
        "# for t in range(time_steps-2, -1, -1):\n",
        "#     y_train_temp[ : , t, : ] = y_train_temp[ : , t+1, : ]\n",
        "#     y_test_temp[ : , t, : ] = y_test_temp[ : , t+1, : ]\n",
        "#     y_val_temp[ : , t, : ] = y_val_temp[ : , t+1, : ]\n",
        "\n",
        "    \n",
        "# # Preview the Before-After results\n",
        "# print( \"======== y_train ========\" )\n",
        "# print( \"\\nShape: before={} after={}\".format( y_train.shape, y_train_temp.shape ) )\n",
        "# print( \"\\nThe first two rows:\" )\n",
        "# print( \"\\tBefore: row[0]: \" , y_train[0] )\n",
        "# print( \"\\tBefore: row[1]: \" , y_train[1] )\n",
        "# print( \"\\n\\tAfter: row[0]: \" , y_train_temp[0] )\n",
        "# print( \"\\tAfter: row[1]: \" , y_train_temp[1] )\n",
        "# #\n",
        "# print( \"\\n======== y_test ========\" )\n",
        "# print( \"\\nShape: before={} after={}\".format( y_test.shape, y_test_temp.shape ) )\n",
        "# print( \"\\nThe first two rows:\" )\n",
        "# print( \"\\tBefore: row[0]: \" , y_test[0] )\n",
        "# print( \"\\tBefore: row[1]: \" , y_test[1] )\n",
        "# print( \"\\n\\tAfter: row[0]: \" , y_test_temp[0] )\n",
        "# print( \"\\tAfter: row[1]: \" , y_test_temp[1] )\n",
        "# #\n",
        "# print( \"\\n======== y_val ========\" )\n",
        "# print( \"\\nShape: before={} after={}\".format( y_val.shape, y_val_temp.shape ) )\n",
        "# print( \"\\nThe first two rows:\" )\n",
        "# print( \"\\tBefore: row[0]: \" , y_val[0] )\n",
        "# print( \"\\tBefore: row[1]: \" , y_val[1] )\n",
        "# print( \"\\n\\tAfter: row[0]: \" , y_val_temp[0] )\n",
        "# print( \"\\tAfter: row[1]: \" , y_val_temp[1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo4zjLKzP4iR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "defe9e5b-aa35-4b13-a667-c04c251494f5"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, ..., 144, 233, 377],\n",
              "       [1, 1, 2, ..., 233, 377, 610],\n",
              "       [1, 2, 3, ..., 377, 610, 987],\n",
              "       ...,\n",
              "       [212207101440105399533740733471, 343358302784187294870275058337,\n",
              "        555565404224292694404015791808, ...,\n",
              "        68330027629092351019822533679447,\n",
              "        110560307156090817237632754212345,\n",
              "        178890334785183168257455287891792],\n",
              "       [343358302784187294870275058337, 555565404224292694404015791808,\n",
              "        898923707008479989274290850145, ...,\n",
              "        110560307156090817237632754212345,\n",
              "        178890334785183168257455287891792,\n",
              "        289450641941273985495088042104137],\n",
              "       [555565404224292694404015791808, 898923707008479989274290850145,\n",
              "        1454489111232772683678306641953, ...,\n",
              "        178890334785183168257455287891792,\n",
              "        289450641941273985495088042104137,\n",
              "        468340976726457153752543329995929]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpJlqgQ6oXn8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "8a0be393-572d-4a36-8b68-0922627d3d9c"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368,\n",
              "       75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309,\n",
              "       3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986,\n",
              "       102334155, 165580141, 267914296, 433494437, 701408733, 1134903170,\n",
              "       1836311903, 2971215073, 4807526976, 7778742049, 12586269025,\n",
              "       20365011074, 32951280099, 53316291173, 86267571272, 139583862445,\n",
              "       225851433717, 365435296162, 591286729879, 956722026041,\n",
              "       1548008755920, 2504730781961, 4052739537881, 6557470319842,\n",
              "       10610209857723, 17167680177565, 27777890035288, 44945570212853,\n",
              "       72723460248141, 117669030460994, 190392490709135, 308061521170129,\n",
              "       498454011879264, 806515533049393, 1304969544928657,\n",
              "       2111485077978050, 3416454622906707, 5527939700884757,\n",
              "       8944394323791464, 14472334024676221, 23416728348467685,\n",
              "       37889062373143906, 61305790721611591, 99194853094755497,\n",
              "       160500643816367088, 259695496911122585, 420196140727489673,\n",
              "       679891637638612258, 1100087778366101931, 1779979416004714189,\n",
              "       2880067194370816120, 4660046610375530309, 7540113804746346429,\n",
              "       12200160415121876738, 19740274219868223167, 31940434634990099905,\n",
              "       51680708854858323072, 83621143489848422977, 135301852344706746049,\n",
              "       218922995834555169026, 354224848179261915075,\n",
              "       573147844013817084101, 927372692193078999176,\n",
              "       1500520536206896083277, 2427893228399975082453,\n",
              "       3928413764606871165730, 6356306993006846248183,\n",
              "       10284720757613717413913, 16641027750620563662096,\n",
              "       26925748508234281076009, 43566776258854844738105,\n",
              "       70492524767089125814114, 114059301025943970552219,\n",
              "       184551825793033096366333, 298611126818977066918552,\n",
              "       483162952612010163284885, 781774079430987230203437,\n",
              "       1264937032042997393488322, 2046711111473984623691759,\n",
              "       3311648143516982017180081, 5358359254990966640871840,\n",
              "       8670007398507948658051921, 14028366653498915298923761,\n",
              "       22698374052006863956975682, 36726740705505779255899443,\n",
              "       59425114757512643212875125, 96151855463018422468774568,\n",
              "       155576970220531065681649693, 251728825683549488150424261,\n",
              "       407305795904080553832073954, 659034621587630041982498215,\n",
              "       1066340417491710595814572169, 1725375039079340637797070384,\n",
              "       2791715456571051233611642553, 4517090495650391871408712937,\n",
              "       7308805952221443105020355490, 11825896447871834976429068427,\n",
              "       19134702400093278081449423917, 30960598847965113057878492344,\n",
              "       50095301248058391139327916261, 81055900096023504197206408605,\n",
              "       131151201344081895336534324866, 212207101440105399533740733471,\n",
              "       343358302784187294870275058337, 555565404224292694404015791808,\n",
              "       898923707008479989274290850145, 1454489111232772683678306641953,\n",
              "       2353412818241252672952597492098, 3807901929474025356630904134051,\n",
              "       6161314747715278029583501626149, 9969216677189303386214405760200,\n",
              "       16130531424904581415797907386349, 26099748102093884802012313146549,\n",
              "       42230279526998466217810220532898, 68330027629092351019822533679447,\n",
              "       110560307156090817237632754212345,\n",
              "       178890334785183168257455287891792,\n",
              "       289450641941273985495088042104137,\n",
              "       468340976726457153752543329995929,\n",
              "       757791618667731139247631372100066], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7F7KbBKXde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "ca2600d2-66a6-4f5c-d939-25051bac0e1a"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#x_minmax_norm = MinMaxScaler().fit( x_train )\n",
        "len_last_x = len(x_train[len(x_train)-1])\n",
        "val_x_max = x_train[len(x_train)-1][len_last_x-1]\n",
        "x_min = np.array([ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ] )\n",
        "x_max = np.array([ val_x_max, val_x_max, val_x_max, val_x_max, val_x_max, val_x_max, val_x_max, val_x_max, val_x_max, val_x_max, val_x_max, val_x_max, val_x_max, val_x_max, val_x_max ] )\n",
        "\n",
        "len_last_x_test = len(x_test[len(x_test)-1])\n",
        "val_x_test_max = x_test[len(x_test)-1][len_last_x_test-1]\n",
        "x_test_max = np.array([ val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max, val_x_test_max ] )\n",
        "\n",
        "print( \"x min-max:\" , x_min , x_max ) \n",
        "print(\"x test max:\", x_test_max)\n",
        "y_minmax_norm = MinMaxScaler().fit( y_train.reshape((y_train.shape[0], 1 )))\n",
        "print( \"y min-max:\" , y_minmax_norm.data_min_ , y_minmax_norm.data_max_ ) \n",
        "\n",
        "y_train [ ... ] = ( y_train [...] - y_minmax_norm.data_min_ ) / ( y_minmax_norm.data_max_ - y_minmax_norm.data_min_ )\n",
        "y_test [ ... ] = ( y_test [...] - y_minmax_norm.data_min_ ) / ( y_minmax_norm.data_max_ - y_minmax_norm.data_min_ )\n",
        "y_val [ ... ] = ( y_val [...] - y_minmax_norm.data_min_ ) / ( y_minmax_norm.data_max_ - y_minmax_norm.data_min_ )\n",
        "\n",
        "x_train[ ... ] = ( x_train [...] - x_min ) / ( x_max - x_min )\n",
        "x_test[ ... ] = ( x_test [...] - x_min ) / ( x_test_max - x_min )\n",
        "x_val[ ... ] = ( x_val [...] - x_min ) / ( x_max - x_min )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x min-max: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [468340976726457153752543329995929 468340976726457153752543329995929\n",
            " 468340976726457153752543329995929 468340976726457153752543329995929\n",
            " 468340976726457153752543329995929 468340976726457153752543329995929\n",
            " 468340976726457153752543329995929 468340976726457153752543329995929\n",
            " 468340976726457153752543329995929 468340976726457153752543329995929\n",
            " 468340976726457153752543329995929 468340976726457153752543329995929\n",
            " 468340976726457153752543329995929 468340976726457153752543329995929\n",
            " 468340976726457153752543329995929]\n",
            "x test max: [7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639\n",
            " 7084593923980518516849609894969925639]\n",
            "y min-max: [610.] [7.57791619e+32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1mQZI4rjRz9"
      },
      "source": [
        "# import pandas as pd\n",
        "# df_x_train = pd.DataFrame(x_train)\n",
        "# df_y_train = pd.DataFrame(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwHVz10sVFJ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "b95dc5c7-7ea4-4963-d383-eeb2b1adef4d"
      },
      "source": [
        "print(\"Dimension before changed:\")\n",
        "print( x_train.shape )\n",
        "print( x_test.shape )\n",
        "print( x_val.shape )\n",
        "\n",
        "x_train = x_train[ ..., np.newaxis ]\n",
        "x_test = x_test[ ..., np.newaxis ]\n",
        "x_val = x_val[ ..., np.newaxis ]\n",
        "\n",
        "print(\"\\nDimension after changed:\")\n",
        "print( x_train.shape )\n",
        "print( x_test.shape )\n",
        "print( x_val.shape )\n",
        "\n",
        "# Convert everything to the default of float32\n",
        "x_train = x_train.astype(np.float32)\n",
        "x_test = x_test.astype(np.float32)\n",
        "x_val = x_val.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "y_val = y_val.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimension before changed:\n",
            "(145, 15)\n",
            "(5, 15)\n",
            "(5, 15)\n",
            "\n",
            "Dimension after changed:\n",
            "(145, 15, 1)\n",
            "(5, 15, 1)\n",
            "(5, 15, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oqpvZtjeHQV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f41a933b-8f9b-452b-a26d-0fe3cf7aa3a4"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "\n",
        "my_rnn = Sequential()\n",
        "\n",
        "# batch_size=None, time_steps=None, input_dim=1\n",
        "my_rnn.add( SimpleRNN( units=30, input_shape=(None,1), return_sequences=True ) )   # RNN layer 1\n",
        "\n",
        "my_rnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_20 (SimpleRNN)    (None, None, 30)          960       \n",
            "=================================================================\n",
            "Total params: 960\n",
            "Trainable params: 960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrhzgsargWPc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "bb38913a-2908-45cb-fdf7-2461f7b7a37f"
      },
      "source": [
        "my_rnn.add( SimpleRNN( units=20, return_sequences=True ) )    # RNN layer 2\n",
        "my_rnn.add( SimpleRNN( units=10 ) )    # RNN layer 3\n",
        "\n",
        "\n",
        "# Use the default 'linear activation'\n",
        "my_rnn.add( Dense(5) )\n",
        "\n",
        "my_rnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_20 (SimpleRNN)    (None, None, 30)          960       \n",
            "_________________________________________________________________\n",
            "simple_rnn_21 (SimpleRNN)    (None, None, 20)          1020      \n",
            "_________________________________________________________________\n",
            "simple_rnn_22 (SimpleRNN)    (None, 10)                310       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 5)                 55        \n",
            "=================================================================\n",
            "Total params: 2,345\n",
            "Trainable params: 2,345\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_hZGXpYga5q"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "\n",
        "adam = Adam(lr=0.01)\n",
        "my_rnn.compile( loss=\"mse\", optimizer=adam, metrics=[mean_squared_error] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukwPDKxNggT3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c5e85ba-56a8-4649-ee84-d040de6f1c75"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint('./RNN_best.hdf5', save_best_only=True, monitor=\"val_loss\", mode='min', save_weights_only=False)\n",
        "hist = my_rnn.fit(x_train, y_train, batch_size=16, epochs=100, callbacks=[checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 0.1814 - mean_squared_error: 0.1814WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1323 - mean_squared_error: 0.1456\n",
            "Epoch 2/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 0.0107 - mean_squared_error: 0.0107WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0090 - mean_squared_error: 0.0097\n",
            "Epoch 3/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 0.0056 - mean_squared_error: 0.0056WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0054 - mean_squared_error: 0.0059\n",
            "Epoch 4/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 0.0025 - mean_squared_error: 0.0025        WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0020 - mean_squared_error: 0.0022\n",
            "Epoch 5/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 0.0017 - mean_squared_error: 0.0017        WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0013 - mean_squared_error: 0.0015\n",
            "Epoch 6/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 9.4685e-04 - mean_squared_error: 9.4685e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.5609e-04 - mean_squared_error: 9.4151e-04\n",
            "Epoch 7/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 7.2600e-04 - mean_squared_error: 7.2600e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.8821e-04 - mean_squared_error: 6.4660e-04\n",
            "Epoch 8/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 4.3014e-04 - mean_squared_error: 4.3014e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.5354e-04 - mean_squared_error: 3.8833e-04\n",
            "Epoch 9/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 3.0737e-04 - mean_squared_error: 3.0737e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 2.7680e-04 - mean_squared_error: 3.0526e-04\n",
            "Epoch 10/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 2.4038e-04 - mean_squared_error: 2.4038e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 2.1799e-04 - mean_squared_error: 2.3883e-04\n",
            "Epoch 11/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 2.4432e-04 - mean_squared_error: 2.4432e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 1.9793e-04 - mean_squared_error: 2.1706e-04\n",
            "Epoch 12/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 2.0189e-04 - mean_squared_error: 2.0189e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 1.8292e-04 - mean_squared_error: 2.0058e-04\n",
            "Epoch 13/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 2.0736e-04 - mean_squared_error: 2.0736e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 1.4966e-04 - mean_squared_error: 1.6362e-04\n",
            "Epoch 14/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 3.8517e-05 - mean_squared_error: 3.8517e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 1.7494e-04 - mean_squared_error: 1.9244e-04\n",
            "Epoch 15/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 2.5714e-04 - mean_squared_error: 2.5714e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 2.3190e-04 - mean_squared_error: 2.5540e-04\n",
            "Epoch 16/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.0800e-04 - mean_squared_error: 1.0800e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 9.7369e-05 - mean_squared_error: 1.0726e-04\n",
            "Epoch 17/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.8509e-04 - mean_squared_error: 1.8509e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 1.6778e-04 - mean_squared_error: 1.8390e-04\n",
            "Epoch 18/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 3.9279e-05 - mean_squared_error: 3.9279e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 2.3644e-04 - mean_squared_error: 2.5761e-04\n",
            "Epoch 19/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 7.1597e-05 - mean_squared_error: 7.1597e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.4652e-05 - mean_squared_error: 6.5682e-05\n",
            "Epoch 20/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 1.9971e-04 - mean_squared_error: 1.9971e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 1.7761e-04 - mean_squared_error: 1.9387e-04\n",
            "Epoch 21/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 1.0787e-04 - mean_squared_error: 1.0787e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.8001e-05 - mean_squared_error: 8.5550e-05\n",
            "Epoch 22/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 1.0060e-04 - mean_squared_error: 1.0060e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 8.3578e-05 - mean_squared_error: 9.1111e-05\n",
            "Epoch 23/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 1.3122e-04 - mean_squared_error: 1.3122e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 9.2501e-05 - mean_squared_error: 1.0196e-04\n",
            "Epoch 24/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 5.8752e-05 - mean_squared_error: 5.8752e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.5264e-05 - mean_squared_error: 6.0452e-05\n",
            "Epoch 25/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.2932e-04 - mean_squared_error: 1.2932e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 1.1979e-04 - mean_squared_error: 1.2866e-04\n",
            "Epoch 26/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.5329e-04 - mean_squared_error: 1.5329e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 1.3857e-04 - mean_squared_error: 1.5228e-04\n",
            "Epoch 27/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 9.9824e-05 - mean_squared_error: 9.9824e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.9874e-05 - mean_squared_error: 9.6968e-05\n",
            "Epoch 28/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.0505e-04 - mean_squared_error: 1.0505e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.6602e-05 - mean_squared_error: 1.0446e-04\n",
            "Epoch 29/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 2.1798e-04 - mean_squared_error: 2.1798e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 1.9783e-04 - mean_squared_error: 2.1659e-04\n",
            "Epoch 30/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.5839e-04 - mean_squared_error: 1.5839e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 1.4693e-04 - mean_squared_error: 1.5760e-04\n",
            "Epoch 31/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 2.3083e-04 - mean_squared_error: 2.3083e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 1.8802e-04 - mean_squared_error: 2.0612e-04\n",
            "Epoch 32/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 3.2596e-04 - mean_squared_error: 3.2596e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 2.6251e-04 - mean_squared_error: 2.8919e-04\n",
            "Epoch 33/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 2.4948e-04 - mean_squared_error: 2.4948e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 2.0216e-04 - mean_squared_error: 2.2250e-04\n",
            "Epoch 34/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 5.1443e-05 - mean_squared_error: 5.1443e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 4.6862e-05 - mean_squared_error: 5.0372e-05\n",
            "Epoch 35/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.8208e-04 - mean_squared_error: 1.8208e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 1.6593e-04 - mean_squared_error: 1.8097e-04\n",
            "Epoch 36/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.5623e-04 - mean_squared_error: 1.5623e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 1.4130e-04 - mean_squared_error: 1.5520e-04\n",
            "Epoch 37/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 5.8797e-05 - mean_squared_error: 5.8797e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.4012e-05 - mean_squared_error: 5.8467e-05\n",
            "Epoch 38/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 2.8672e-05 - mean_squared_error: 2.8672e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.3741e-05 - mean_squared_error: 7.0123e-05\n",
            "Epoch 39/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 5.3102e-05 - mean_squared_error: 5.3102e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 4.3069e-05 - mean_squared_error: 4.7315e-05\n",
            "Epoch 40/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 2.9077e-05 - mean_squared_error: 2.9077e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 5.6938e-05 - mean_squared_error: 5.3038e-05\n",
            "Epoch 41/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.0681e-04 - mean_squared_error: 1.0681e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 1.0022e-04 - mean_squared_error: 1.0636e-04\n",
            "Epoch 42/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 5.1842e-05 - mean_squared_error: 5.1842e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 5.6161e-05 - mean_squared_error: 5.9680e-05\n",
            "Epoch 43/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 3.0366e-05 - mean_squared_error: 3.0366e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 4.4333e-05 - mean_squared_error: 4.6730e-05\n",
            "Epoch 44/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 3.0415e-05 - mean_squared_error: 3.0415e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 3.2943e-05 - mean_squared_error: 3.5644e-05\n",
            "Epoch 45/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 5.2699e-05 - mean_squared_error: 5.2699e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 4.3819e-05 - mean_squared_error: 4.7429e-05\n",
            "Epoch 46/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 3.4896e-05 - mean_squared_error: 3.4896e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.2392e-05 - mean_squared_error: 9.0080e-05\n",
            "Epoch 47/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 1.9159e-04 - mean_squared_error: 1.9159e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 1.3691e-04 - mean_squared_error: 1.5039e-04\n",
            "Epoch 48/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 2.0139e-04 - mean_squared_error: 2.0139e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 1.6726e-04 - mean_squared_error: 1.8424e-04\n",
            "Epoch 49/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 1.2810e-04 - mean_squared_error: 1.2810e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 1.0923e-04 - mean_squared_error: 1.1957e-04\n",
            "Epoch 50/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 4.7483e-05 - mean_squared_error: 4.7483e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 4.4130e-05 - mean_squared_error: 4.7252e-05\n",
            "Epoch 51/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 3.9895e-05 - mean_squared_error: 3.9895e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 3.6003e-05 - mean_squared_error: 3.9626e-05\n",
            "Epoch 52/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 4.4427e-05 - mean_squared_error: 4.4427e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 4.0700e-05 - mean_squared_error: 4.4170e-05\n",
            "Epoch 53/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 2.9172e-05 - mean_squared_error: 2.9172e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.6721e-05 - mean_squared_error: 2.7775e-05\n",
            "Epoch 54/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 9.8840e-05 - mean_squared_error: 9.8840e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.5512e-05 - mean_squared_error: 8.9569e-05\n",
            "Epoch 55/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 4.9361e-05 - mean_squared_error: 4.9361e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 4.0678e-05 - mean_squared_error: 4.2774e-05\n",
            "Epoch 56/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 3.5892e-05 - mean_squared_error: 3.5892e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.2514e-05 - mean_squared_error: 3.5659e-05\n",
            "Epoch 57/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 2.0179e-05 - mean_squared_error: 2.0179e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 5.8482e-05 - mean_squared_error: 6.0617e-05\n",
            "Epoch 58/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 4.5112e-05 - mean_squared_error: 4.5112e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.3310e-05 - mean_squared_error: 6.9567e-05\n",
            "Epoch 59/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.7574e-05 - mean_squared_error: 1.7574e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 1.5973e-05 - mean_squared_error: 1.7463e-05\n",
            "Epoch 60/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 5.1752e-05 - mean_squared_error: 5.1752e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 3.8384e-05 - mean_squared_error: 4.1905e-05\n",
            "Epoch 61/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 1.0021e-04 - mean_squared_error: 1.0021e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.1901e-05 - mean_squared_error: 8.9410e-05\n",
            "Epoch 62/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 5.3910e-05 - mean_squared_error: 5.3910e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.4120e-05 - mean_squared_error: 4.8173e-05\n",
            "Epoch 63/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 2.6885e-05 - mean_squared_error: 2.6885e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 2.4917e-05 - mean_squared_error: 2.6750e-05\n",
            "Epoch 64/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 7.4783e-05 - mean_squared_error: 7.4783e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 6.8394e-05 - mean_squared_error: 7.4342e-05\n",
            "Epoch 65/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 4.2590e-05 - mean_squared_error: 4.2590e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.0707e-04 - mean_squared_error: 6.0830e-05\n",
            "Epoch 66/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0033 - mean_squared_error: 0.0033        WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0030 - mean_squared_error: 0.0033\n",
            "Epoch 67/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 0.0018 - mean_squared_error: 0.0018        WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0013 - mean_squared_error: 0.0014\n",
            "Epoch 68/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 6.6578e-04 - mean_squared_error: 6.6578e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 6.0513e-04 - mean_squared_error: 6.6160e-04\n",
            "Epoch 69/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 4.4546e-04 - mean_squared_error: 4.4546e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 3.4341e-04 - mean_squared_error: 3.7235e-04\n",
            "Epoch 70/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 1.2195e-04 - mean_squared_error: 1.2195e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 1.0101e-04 - mean_squared_error: 1.0905e-04\n",
            "Epoch 71/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 9.3390e-05 - mean_squared_error: 9.3390e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.5740e-05 - mean_squared_error: 8.2751e-05\n",
            "Epoch 72/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 3.7646e-05 - mean_squared_error: 3.7646e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 1.0452e-04 - mean_squared_error: 1.1372e-04\n",
            "Epoch 73/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 6.7544e-05 - mean_squared_error: 6.7544e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0017 - mean_squared_error: 2.0058e-04\n",
            "Epoch 74/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 0.0114 - mean_squared_error: 0.0114WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0093 - mean_squared_error: 0.0101\n",
            "Epoch 75/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 0.0129 - mean_squared_error: 0.0129WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0101 - mean_squared_error: 0.0109\n",
            "Epoch 76/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 0.0083 - mean_squared_error: 0.0083WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0071 - mean_squared_error: 0.0078\n",
            "Epoch 77/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 0.0055 - mean_squared_error: 0.0055WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0041 - mean_squared_error: 0.0045\n",
            "Epoch 78/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 0.0022 - mean_squared_error: 0.0022WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0018 - mean_squared_error: 0.0019\n",
            "Epoch 79/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 0.0011 - mean_squared_error: 0.0011        WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 9.3674e-04 - mean_squared_error: 0.0010\n",
            "Epoch 80/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 9.1999e-04 - mean_squared_error: 9.1999e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.3650e-04 - mean_squared_error: 8.0802e-04\n",
            "Epoch 81/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 7.9482e-04 - mean_squared_error: 7.9482e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.6292e-04 - mean_squared_error: 7.1725e-04\n",
            "Epoch 82/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 5.6040e-04 - mean_squared_error: 5.6040e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.5734e-04 - mean_squared_error: 5.0103e-04\n",
            "Epoch 83/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 5.2467e-04 - mean_squared_error: 5.2467e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 3.7645e-04 - mean_squared_error: 4.1265e-04\n",
            "Epoch 84/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 3.9230e-04 - mean_squared_error: 3.9230e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.1948e-04 - mean_squared_error: 3.4977e-04\n",
            "Epoch 85/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 3.5863e-04 - mean_squared_error: 3.5863e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 3.2430e-04 - mean_squared_error: 3.5626e-04\n",
            "Epoch 86/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 2.8419e-04 - mean_squared_error: 2.8419e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 2.5766e-04 - mean_squared_error: 2.8236e-04\n",
            "Epoch 87/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 2.8711e-04 - mean_squared_error: 2.8711e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 2.5921e-04 - mean_squared_error: 2.8519e-04\n",
            "Epoch 88/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 2.9734e-04 - mean_squared_error: 2.9734e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 2.1003e-04 - mean_squared_error: 2.3140e-04\n",
            "Epoch 89/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 2.6271e-04 - mean_squared_error: 2.6271e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 2.1330e-04 - mean_squared_error: 2.2993e-04\n",
            "Epoch 90/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 7.4256e-05 - mean_squared_error: 7.4256e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 2.1056e-04 - mean_squared_error: 2.2522e-04\n",
            "Epoch 91/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 3.0543e-04 - mean_squared_error: 3.0543e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 2.6368e-04 - mean_squared_error: 2.7494e-04\n",
            "Epoch 92/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 2.6732e-04 - mean_squared_error: 2.6732e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 2.4153e-04 - mean_squared_error: 2.1845e-04\n",
            "Epoch 93/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 4.5104e-04 - mean_squared_error: 4.5104e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 3.4999e-04 - mean_squared_error: 3.8041e-04\n",
            "Epoch 94/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 2.4874e-04 - mean_squared_error: 2.4874e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 1.9697e-04 - mean_squared_error: 2.1046e-04\n",
            "Epoch 95/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 1.6435e-04 - mean_squared_error: 1.6435e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 1.3223e-04 - mean_squared_error: 1.4561e-04\n",
            "Epoch 96/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.1502e-04 - mean_squared_error: 1.1502e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 1.0375e-04 - mean_squared_error: 1.1424e-04\n",
            "Epoch 97/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 4.2213e-05 - mean_squared_error: 4.2213e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 1.1191e-04 - mean_squared_error: 1.2267e-04\n",
            "Epoch 98/100\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.2852e-04 - mean_squared_error: 1.2852e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 1.1746e-04 - mean_squared_error: 1.2776e-04\n",
            "Epoch 99/100\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 4.8378e-05 - mean_squared_error: 4.8378e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 1.0061e-04 - mean_squared_error: 1.0738e-04\n",
            "Epoch 100/100\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 9.2158e-05 - mean_squared_error: 9.2158e-05WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.7381e-05 - mean_squared_error: 9.5749e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM-3SwIbg1Rt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ef2df9b9-424b-4a12-cf08-8b5ee711f66a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline                \n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['mean_squared_error'])\n",
        "plt.title('Model loss: mean squared error')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5QRhZmZt/X9oaDUsagH1vQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczV-6Pjg9p2"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Evaluate the model from the last epoch\n",
        "score = my_rnn.evaluate( x_test, y_test, verbose=0 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mDIh2ezhXcc"
      },
      "source": [
        "y_test_predict = my_rnn.predict( x_test ) # last epoch\n",
        "\n",
        "# Denormalize the value\n",
        "y_inv = y_minmax_norm.inverse_transform( y_test_predict )\n",
        "\n",
        "# Round all float values to their closest integers\n",
        "y_inv = np.around( y_inv )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey-3E75t4NxH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd846689-7929-41f1-aa08-1912646da88f"
      },
      "source": [
        "x_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1.7307028e-04],\n",
              "        [2.8003359e-04],\n",
              "        [4.5310386e-04],\n",
              "        [7.3313742e-04],\n",
              "        [1.1862413e-03],\n",
              "        [1.9193788e-03],\n",
              "        [3.1056199e-03],\n",
              "        [5.0249989e-03],\n",
              "        [8.1306184e-03],\n",
              "        [1.3155618e-02],\n",
              "        [2.1286236e-02],\n",
              "        [3.4441855e-02],\n",
              "        [5.5728089e-02],\n",
              "        [9.0169944e-02],\n",
              "        [1.4589803e-01]],\n",
              "\n",
              "       [[2.8003359e-04],\n",
              "        [4.5310386e-04],\n",
              "        [7.3313742e-04],\n",
              "        [1.1862413e-03],\n",
              "        [1.9193788e-03],\n",
              "        [3.1056199e-03],\n",
              "        [5.0249989e-03],\n",
              "        [8.1306184e-03],\n",
              "        [1.3155618e-02],\n",
              "        [2.1286236e-02],\n",
              "        [3.4441855e-02],\n",
              "        [5.5728089e-02],\n",
              "        [9.0169944e-02],\n",
              "        [1.4589803e-01],\n",
              "        [2.3606798e-01]],\n",
              "\n",
              "       [[4.5310386e-04],\n",
              "        [7.3313742e-04],\n",
              "        [1.1862413e-03],\n",
              "        [1.9193788e-03],\n",
              "        [3.1056199e-03],\n",
              "        [5.0249989e-03],\n",
              "        [8.1306184e-03],\n",
              "        [1.3155618e-02],\n",
              "        [2.1286236e-02],\n",
              "        [3.4441855e-02],\n",
              "        [5.5728089e-02],\n",
              "        [9.0169944e-02],\n",
              "        [1.4589803e-01],\n",
              "        [2.3606798e-01],\n",
              "        [3.8196602e-01]],\n",
              "\n",
              "       [[7.3313742e-04],\n",
              "        [1.1862413e-03],\n",
              "        [1.9193788e-03],\n",
              "        [3.1056199e-03],\n",
              "        [5.0249989e-03],\n",
              "        [8.1306184e-03],\n",
              "        [1.3155618e-02],\n",
              "        [2.1286236e-02],\n",
              "        [3.4441855e-02],\n",
              "        [5.5728089e-02],\n",
              "        [9.0169944e-02],\n",
              "        [1.4589803e-01],\n",
              "        [2.3606798e-01],\n",
              "        [3.8196602e-01],\n",
              "        [6.1803401e-01]],\n",
              "\n",
              "       [[1.1862413e-03],\n",
              "        [1.9193788e-03],\n",
              "        [3.1056199e-03],\n",
              "        [5.0249989e-03],\n",
              "        [8.1306184e-03],\n",
              "        [1.3155618e-02],\n",
              "        [2.1286236e-02],\n",
              "        [3.4441855e-02],\n",
              "        [5.5728089e-02],\n",
              "        [9.0169944e-02],\n",
              "        [1.4589803e-01],\n",
              "        [2.3606798e-01],\n",
              "        [3.8196602e-01],\n",
              "        [6.1803401e-01],\n",
              "        [1.0000000e+00]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ngnQ1FpihUp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7d269f61-3a92-466b-d726-f9993b8195e0"
      },
      "source": [
        "y_test_predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.17421532, 0.15366626, 0.17595239, 0.15701266, 0.1549849 ],\n",
              "       [0.2803684 , 0.2520463 , 0.28201836, 0.25671044, 0.25387347],\n",
              "       [0.4433234 , 0.41235945, 0.4421344 , 0.41696122, 0.41499624],\n",
              "       [0.6702673 , 0.65671897, 0.6592938 , 0.657439  , 0.6608153 ],\n",
              "       [0.91836023, 0.95305055, 0.88879025, 0.94280446, 0.96245944]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h53zC3NnspDH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f6e80ba9-21ab-426d-f7ee-e61924b9f101"
      },
      "source": [
        "y_inv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.3201891e+32, 1.1644700e+32, 1.3333524e+32, 1.1898288e+32,\n",
              "        1.1744626e+32],\n",
              "       [2.1246082e+32, 1.9099856e+32, 2.1371116e+32, 1.9453301e+32,\n",
              "        1.9238319e+32],\n",
              "       [3.3594675e+32, 3.1248253e+32, 3.3504576e+32, 3.1596973e+32,\n",
              "        3.1448068e+32],\n",
              "       [5.0792293e+32, 4.9765611e+32, 4.9960730e+32, 4.9820177e+32,\n",
              "        5.0076029e+32],\n",
              "       [6.9592571e+32, 7.2221375e+32, 6.7351784e+32, 7.1444932e+32,\n",
              "        7.2934367e+32]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHZtH9YyqIC_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKwiOpVy6sZ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fd08264-0f37-497d-f579-9d238d25eba4"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 15, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 409
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO9AMhdCMqJm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae694dd8-985a-45e9-bde1-22adbf5b9736"
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 410
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwbjvzan6sdy"
      },
      "source": [
        "# test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoZr1kIoJG0F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}